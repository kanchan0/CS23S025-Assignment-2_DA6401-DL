{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part-A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:01:19.812382Z",
     "iopub.status.busy": "2025-04-16T18:01:19.812046Z",
     "iopub.status.idle": "2025-04-16T18:02:42.701928Z",
     "shell.execute_reply": "2025-04-16T18:02:42.701372Z",
     "shell.execute_reply.started": "2025-04-16T18:01:19.812358Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightning\n",
      "  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1+cu124)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.13.1)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.16)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.19.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\n",
      "Downloading lightning-2.5.1-py3-none-any.whl (818 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lightning\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
      "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
      "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
      "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
      "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed lightning-2.5.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23s025\u001b[0m (\u001b[33mcs23s025-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install lightning\n",
    "import wandb\n",
    "\n",
    "wandb.login(key = \"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:02:42.703410Z",
     "iopub.status.busy": "2025-04-16T18:02:42.703103Z",
     "iopub.status.idle": "2025-04-16T18:02:53.838809Z",
     "shell.execute_reply": "2025-04-16T18:02:53.838066Z",
     "shell.execute_reply.started": "2025-04-16T18:02:42.703393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning.pytorch as L\n",
    "\n",
    "\n",
    "class convNet(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size,\n",
    "        activation,\n",
    "        num_filters,\n",
    "        filter_size,\n",
    "        filter_org,\n",
    "        stride,\n",
    "        padding,\n",
    "        dense_neurons,\n",
    "        learning_rate,\n",
    "        optimizer,\n",
    "        dropout,\n",
    "        usedropout,\n",
    "        batchnorm,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # self.img_size = img_size\n",
    "        self.activation = activation\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dense_neurons = dense_neurons\n",
    "        self.lr = learning_rate\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        self.usedropout = usedropout\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        # Define convolutional layers\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=3,\n",
    "            out_channels=self.num_filters,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(num_features=self.num_filters)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=self.num_filters,\n",
    "            out_channels=self.num_filters * filter_org,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(num_features=self.conv2.out_channels)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=self.conv2.out_channels,\n",
    "            out_channels=self.conv2.out_channels * filter_org,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.bn3 = nn.BatchNorm2d(num_features=self.conv3.out_channels)\n",
    "\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=self.conv3.out_channels,\n",
    "            out_channels=self.num_filters * filter_org,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.bn4 = nn.BatchNorm2d(num_features=self.conv4.out_channels)\n",
    "\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=self.conv4.out_channels,\n",
    "            out_channels=self.num_filters * filter_org,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.bn5 = nn.BatchNorm2d(num_features=self.conv5.out_channels)\n",
    "\n",
    "        # Define activation function based on user input\n",
    "        if self.activation.lower() == \"relu\":\n",
    "            self.activation_layer = nn.ReLU()\n",
    "        elif self.activation.lower() == \"gelu\":\n",
    "            self.activation_layer = nn.GELU()\n",
    "        elif self.activation.lower() == \"silu\":\n",
    "            self.activation_layer = nn.SiLU()\n",
    "        elif self.activation.lower() == \"mish\":\n",
    "            self.activation_layer = nn.Mish()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid activation function. Choose from 'relu', 'gelu', 'mish' or 'silu'\"\n",
    "            )\n",
    "\n",
    "        # Define max-pooling layers\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout_layer = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        c1s = int(\n",
    "            (((img_size - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n",
    "        )\n",
    "\n",
    "        c2s = int(\n",
    "            (((c1s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n",
    "        )\n",
    "\n",
    "        c3s = int(\n",
    "            (((c2s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n",
    "        )\n",
    "\n",
    "        c4s = int(\n",
    "            (((c3s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n",
    "        )\n",
    "\n",
    "        c5s = int(\n",
    "            (((c4s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n",
    "        )\n",
    "\n",
    "        # print(f\"c1s_{c1s}_c2s_{c2s}_c3s_{c3s}_c4s_{c4s}_c5s_{c5s}_out_{self.conv5.out_channels}\")\n",
    "\n",
    "        flatten_neurons = int(c5s * c5s * self.conv5.out_channels)\n",
    "\n",
    "        # print(f\"multi_{c5s*c5s*self.conv5.out_channels}_flat_{flatten_neurons}\")\n",
    "\n",
    "        # Define fully connected layers\n",
    "        self.fc1 = nn.Linear(flatten_neurons, dense_neurons)\n",
    "        self.fc2 = nn.Linear(dense_neurons, 10)\n",
    "        self.softmax_layer = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolutional layers\n",
    "        x = self.conv1(x)\n",
    "        if self.batchnorm == \"Y\":\n",
    "            x = self.bn1(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        if self.batchnorm == \"Y\":\n",
    "            x = self.bn2(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        if self.usedropout == \"Y\":\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        if self.batchnorm == \"Y\":\n",
    "            x = self.bn3(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        if self.usedropout == \"Y\":\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "        if self.batchnorm == \"Y\":\n",
    "            x = self.bn4(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        if self.usedropout == \"Y\":\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        if self.batchnorm == \"Y\":\n",
    "            x = self.bn5(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        # Flatten\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.usedropout == \"Y\":\n",
    "            x = self.dropout_layer(x)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.activation_layer(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.softmax_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"train_loss\": loss, \"train_accuracy\": accuracy},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\"validation_loss\": loss, \"validation_accuracy\": accuracy},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
    "\n",
    "        self.log_dict(\n",
    "            {\"test_loss\": loss, \"test_accuracy\": accuracy},\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        accuracy = 0\n",
    "\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        y_pred = torch.argmax(torch.softmax(scores, dim=1), dim=1)\n",
    "        accuracy += (y_pred == y).sum().item()\n",
    "        accuracy = accuracy / len(scores)\n",
    "        return loss, scores, y, accuracy\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "\n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer.lower() == \"adam\":\n",
    "\n",
    "            return optim.Adam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        elif self.optimizer.lower() == \"sgd\":\n",
    "\n",
    "            return optim.SGD(self.parameters(), lr=self.lr)\n",
    "\n",
    "        elif self.optimizer.lower() == \"nadam\":\n",
    "\n",
    "            return optim.NAdam(self.parameters(), lr=self.lr)\n",
    "\n",
    "        elif self.optimizer.lower() == \"rmsprop\":\n",
    "\n",
    "            return optim.RMSprop(self.parameters(), lr=self.lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:02:53.840049Z",
     "iopub.status.busy": "2025-04-16T18:02:53.839627Z",
     "iopub.status.idle": "2025-04-16T18:02:53.849946Z",
     "shell.execute_reply": "2025-04-16T18:02:53.849270Z",
     "shell.execute_reply.started": "2025-04-16T18:02:53.840029Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as L\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class iNaturalistDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers, img_size, data_augmentation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = data_dir / \"train\"\n",
    "        self.test_path = data_dir / \"val\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        if data_augmentation == \"Y\":\n",
    "\n",
    "            self.data_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size=(img_size, img_size)),\n",
    "                    transforms.AutoAugment(),  # This is the data augmentation method chosen\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n",
    "                    ),  # These values are calculated for our dataset\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif data_augmentation == \"N\":\n",
    "\n",
    "            self.data_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size=(img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.test_data_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=(img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.4748, 0.4645, 0.3965], std=[0.2004, 0.1954, 0.1923]\n",
    "                ),\n",
    "            ] # trainer.test(model, data)\n",
    "        )\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "\n",
    "            # First load all the training data\n",
    "            train_data_full = datasets.ImageFolder(\n",
    "                root=self.train_path,\n",
    "                transform=self.data_transform,\n",
    "                target_transform=None,\n",
    "            )\n",
    "\n",
    "            # Decide the number of validation samples required\n",
    "            validation_samples_per_class = int(0.2 * 1000)\n",
    "\n",
    "            # These lists will hold the indices of training and validation data samples\n",
    "            train_indices = []\n",
    "            val_indices = []\n",
    "\n",
    "            for class_idx in range(len(train_data_full.classes)):\n",
    "\n",
    "                # Obtain the indices of each class\n",
    "                class_indices = [\n",
    "                    idx\n",
    "                    for idx, (_, label) in enumerate(train_data_full.imgs)\n",
    "                    if label == class_idx\n",
    "                ]\n",
    "\n",
    "                # Split and add the indices to the respective lists\n",
    "                val_indices.extend(class_indices[:validation_samples_per_class])\n",
    "                train_indices.extend(class_indices[validation_samples_per_class:])\n",
    "\n",
    "            # Create a two subsets of the initially loaded training data as training data and validation data\n",
    "            self.train_data = Subset(train_data_full, train_indices)\n",
    "            self.val_data = Subset(train_data_full, val_indices)\n",
    "\n",
    "        if stage == \"test\":\n",
    "            # Load the test data\n",
    "            self.test_data = datasets.ImageFolder(\n",
    "                root=self.test_path, transform=self.test_data_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:02:53.851959Z",
     "iopub.status.busy": "2025-04-16T18:02:53.851698Z",
     "iopub.status.idle": "2025-04-16T18:02:53.873316Z",
     "shell.execute_reply": "2025-04-16T18:02:53.872638Z",
     "shell.execute_reply.started": "2025-04-16T18:02:53.851937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class wandbconfig:\n",
    "\n",
    "    def set_configs(args):\n",
    "\n",
    "        wandb_configs = {\n",
    "            \"epochs\": args.epochs,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"dataug\": args.dataug,\n",
    "            \"batchnorm\": args.batchnorm,\n",
    "            \"num_filters\": args.num_filters,\n",
    "            \"filter_size\": args.filter_size,\n",
    "            \"filter_org\": args.filter_org,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"dropout\": args.dropout,\n",
    "            \"usedropout\": args.usedropout,\n",
    "            \"optimizer\": args.optimizer,\n",
    "            \"dense_neurons\": args.dense_neurons,\n",
    "            \"activation\": args.activation,\n",
    "            \"stride\": args.stride,\n",
    "            \"padding\": args.padding,\n",
    "        }\n",
    "\n",
    "        return wandb_configs\n",
    "\n",
    "    def run_name(wandb_configs):\n",
    "\n",
    "        run_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}_da_{}\".format(\n",
    "            wandb_configs[\"num_filters\"],\n",
    "            wandb_configs[\"filter_size\"],\n",
    "            wandb_configs[\"filter_org\"],\n",
    "            wandb_configs[\"activation\"],\n",
    "            wandb_configs[\"epochs\"],\n",
    "            wandb_configs[\"batch_size\"],\n",
    "            wandb_configs[\"dense_neurons\"],\n",
    "            wandb_configs[\"dataug\"],\n",
    "        )\n",
    "\n",
    "        return run_name\n",
    "\n",
    "    def sweep_name(wandb_configs):\n",
    "\n",
    "        sweep_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}\".format(\n",
    "            wandb_configs.num_filters,\n",
    "            wandb_configs.filter_size,\n",
    "            wandb_configs.filter_org,\n",
    "            wandb_configs.activation,\n",
    "            wandb_configs.epochs,\n",
    "            wandb_configs.batch_size,\n",
    "            wandb_configs.dense_neurons,\n",
    "        )\n",
    "\n",
    "        return sweep_name\n",
    "\n",
    "    def tuner_set_configs(args):\n",
    "\n",
    "        wandb_configs = {\n",
    "            \"epochs\": args.epochs,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"dropout\": args.dropout,\n",
    "        }\n",
    "\n",
    "        return wandb_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T18:02:53.874342Z",
     "iopub.status.busy": "2025-04-16T18:02:53.874084Z",
     "iopub.status.idle": "2025-04-16T19:59:52.870324Z",
     "shell.execute_reply": "2025-04-16T19:59:52.869636Z",
     "shell.execute_reply.started": "2025-04-16T18:02:53.874326Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: 0bgpvbyq\n",
      "Sweep URL: https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hy9lwti5 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_180300-hy9lwti5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hy9lwti5' target=\"_blank\">smooth-sweep-1</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hy9lwti5' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hy9lwti5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 9.7 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 819 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 3.3 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 3.3 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 1.6 M  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 65.6 K | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "9.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.1 M     Total params\n",
      "36.365    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "699b0cd9aca6486985af79c6cd2de175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d54183e8b614471b9224fe8612f8363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    validation_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.398499995470047     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.6055374145507812     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   validation_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.398499995470047    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     validation_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.6055374145507812    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▆▅▅▄▄▄▃▃▂▂▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▃▅▆▅▄▅▇▆▅▅▆▇▇▅▆███</td></tr><tr><td>validation_loss</td><td>▃▃▄▃▁▁▂▃▂▁▁▅▄▃▄▆▇█▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.87886</td></tr><tr><td>train_loss</td><td>0.35896</td></tr><tr><td>trainer/global_step</td><td>1260</td></tr><tr><td>validation_accuracy</td><td>0.3985</td></tr><tr><td>validation_loss</td><td>2.60554</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hy9lwti5' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hy9lwti5</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_180300-hy9lwti5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: a57p65bp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_182346-a57p65bp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/a57p65bp' target=\"_blank\">avid-sweep-2</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/a57p65bp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/a57p65bp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.0 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.204    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d4046659f4a4e47a7a9ecd3530c0fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f8072ad00cc4e08b1ab72c0e1468f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/a57p65bp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/a57p65bp</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_182346-a57p65bp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run a57p65bp errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "    loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 140, in forward\n",
      "    x = self.activation_layer(x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 432, in forward\n",
      "    return F.silu(input, inplace=self.inplace)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2380, in silu\n",
      "    return torch._C._nn.silu(input)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.89 GiB of which 3.47 GiB is free. Process 3444 has 12.42 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run a57p65bp errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._step_fn()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 140, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.activation_layer(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 432, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.silu(input, inplace=self.inplace)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2380, in silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch._C._nn.silu(input)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 15.89 GiB of which 3.47 GiB is free. Process 3444 has 12.42 GiB memory in use. Of the allocated memory 8.12 GiB is allocated by PyTorch, and 4.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vytmv6jd with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_182417-vytmv6jd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vytmv6jd' target=\"_blank\">dauntless-sweep-3</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vytmv6jd' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vytmv6jd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 9.7 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 819 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 3.3 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 3.3 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 1.6 M  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.152    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32bfe0c8aa9d4f6581aeab08eceebb41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43655519601412498eb441ed2924243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee5539549dd447d8af3962f6908954f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dc141b4fb54c5b83be0e9daadadb43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8688559a6ed4c69a649d436730971a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47a22cf40e3040eab2b57c66acab8ab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4346ccdf2704ad48655440b9a50007d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31e2bcd57074021bd1a1e6b5d99759e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cad0e5a9d3a54020a1ffbb7fefef60a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f501260f99a04d1da7e7ac70e5931c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609e3f0a5c0944388abf3193ce22e078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11854d8537bf4c4791599936444986b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d0f35e6b5c4429b13a0243c04f0177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7319d8ae461b4dddb99b5868291de6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bef89d7456740a3b04cfd4a63651345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e14a1995382d4b2f9281f16667d3c042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34336c3a4d1144a8b88c6ba3dc01d631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cab9ff22fc949f8b7e8b2f1bbcfa7b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2102ff7924744c3ba6f7e9b5ae997c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c09f947845a4c8c9800e8c9fd51effd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98e232b9fa97414cb2271659a60c3f08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89285ab82d6640a5ab6610881ccba030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05314e7f2a6941ddae3dddea4dcfa1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    validation_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.38600000739097595    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.4878222942352295     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   validation_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.38600000739097595   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     validation_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4878222942352295    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▃▃▃▃▄▄▄▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▄▅▆▄▆▇▆▆█▇█▆█▇▇███</td></tr><tr><td>validation_loss</td><td>▄▄▄▂▂▂▃▂▁▂▃▂▂▃▄▅▅▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.86248</td></tr><tr><td>train_loss</td><td>0.42329</td></tr><tr><td>trainer/global_step</td><td>2500</td></tr><tr><td>validation_accuracy</td><td>0.386</td></tr><tr><td>validation_loss</td><td>2.48782</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vytmv6jd' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vytmv6jd</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_182417-vytmv6jd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4bviathp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_184226-4bviathp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4bviathp' target=\"_blank\">fresh-sweep-4</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4bviathp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4bviathp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c27aea7f1b744a5bb4da9932b216bc64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3150c048e8764a5e91a17675bf6165d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e23fd7ec928d488f8e5411e03be876e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b723635e936429c8c2643595b743b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁██</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁██</td></tr><tr><td>validation_accuracy</td><td>▁█</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.25641</td></tr><tr><td>train_loss</td><td>2.05423</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>validation_accuracy</td><td>0.2515</td></tr><tr><td>validation_loss</td><td>2.05566</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4bviathp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4bviathp</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_184226-4bviathp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4x3lete7 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_184514-4x3lete7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4x3lete7' target=\"_blank\">olive-sweep-5</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4x3lete7' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4x3lete7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 9.7 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 819 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 3.3 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 3.3 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 1.6 M  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.152    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eb975ca23d42528ea0465ac78fa5de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa848572f7cf4e83b2f3c4c2d2027b9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c04f7b9f90d40e0ad05261ffb363051",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5415f4c8a0b4657924b5ca288f4249e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁██</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁██</td></tr><tr><td>validation_accuracy</td><td>▁█</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.21553</td></tr><tr><td>train_loss</td><td>2.18537</td></tr><tr><td>trainer/global_step</td><td>125</td></tr><tr><td>validation_accuracy</td><td>0.2145</td></tr><tr><td>validation_loss</td><td>2.17156</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4x3lete7' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/4x3lete7</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_184514-4x3lete7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ljfqrmvf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_184812-ljfqrmvf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ljfqrmvf' target=\"_blank\">distinctive-sweep-6</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ljfqrmvf' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ljfqrmvf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13b0656817e466d9af63f77233ea0ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770e19b9bc5a436298e5f0aaeb38a27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c6ddae07674642b68a6fb830bcebba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c596f92be9454689aa40c3e0ba731adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f7d02b4a20490c8d1bb8909d28cacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84c28d136fc49fea127ef4756eaaf5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab1c9f6ea95b43b0ab01b3d8a1b28bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6d64e75198467a97f5991a6634af7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3fa5dedade94988a48c590c854cbcad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "153561284ae84673bffe891680e8c9c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b92fb300274ac389e7dcfd95bb51e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e622115d8dd45978772af4652b39a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00b2e98d45b74c1d8f5864785c48733f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a685fdca329142c0bf49b1d9da1e8425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa2d25e24c7402d958e51425c25088e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "790bf7a1a9404ba99280db5da1ad33f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa56b78843cf4f9a87d12c7ec75cf2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b3333be25594d36ab31734c953aa2cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bec1d64ab404f0088fcee73c1ef9ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d2974463c3483a8f48e8df12ae0775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a533b3679ca64103be11a15826d3c4d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be295fa45d04c2cbf9de89f8453b9af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072ca30cbb20443d8b49d014e0b7f0c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    validation_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.38499999046325684    </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.4788975715637207     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m   validation_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.38499999046325684   \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m     validation_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.4788975715637207    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▅▅▅▄▄▄▃▃▂▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▆▅▆▅▇▆▆▇█▆█▇▇█▇▇▇▇</td></tr><tr><td>validation_loss</td><td>▃▃▂▁▁▁▂▁▁▂▂▁▃▂▄▅▅████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.87786</td></tr><tr><td>train_loss</td><td>0.3622</td></tr><tr><td>trainer/global_step</td><td>1260</td></tr><tr><td>validation_accuracy</td><td>0.385</td></tr><tr><td>validation_loss</td><td>2.4789</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ljfqrmvf' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ljfqrmvf</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_184812-ljfqrmvf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: moe2thoh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_190526-moe2thoh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/moe2thoh' target=\"_blank\">wobbly-sweep-7</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/moe2thoh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/moe2thoh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44c1547d77146119f12d80995290328",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0b5f7d028df4d39aa483cc252404533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac8aa4b66e044a30bca05d0bf48a77d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "487f6d5417f741daa7401e465ca798db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8767f156a542088736372706653439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁██</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁██</td></tr><tr><td>validation_accuracy</td><td>▁█</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.28516</td></tr><tr><td>train_loss</td><td>2.01252</td></tr><tr><td>trainer/global_step</td><td>125</td></tr><tr><td>validation_accuracy</td><td>0.2385</td></tr><tr><td>validation_loss</td><td>2.14862</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/moe2thoh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/moe2thoh</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_190526-moe2thoh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 76m90rdp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_190814-76m90rdp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76m90rdp' target=\"_blank\">pretty-sweep-8</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76m90rdp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76m90rdp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.0 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.204    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa058a5dd0454903bafbfc2d1cd83fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfa6c0d649d14519b876dcc5656c3a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf31d4482d94dc18d7eaec96d80d275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00a7a01d48e045bc8d90d486f41e5893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁██</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>█▁</td></tr><tr><td>trainer/global_step</td><td>▁▁██</td></tr><tr><td>validation_accuracy</td><td>▁█</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.1934</td></tr><tr><td>train_loss</td><td>2.18709</td></tr><tr><td>trainer/global_step</td><td>499</td></tr><tr><td>validation_accuracy</td><td>0.192</td></tr><tr><td>validation_loss</td><td>2.17685</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_relu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76m90rdp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76m90rdp</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_190814-76m90rdp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 6i0vykwk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_191218-6i0vykwk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/6i0vykwk' target=\"_blank\">elated-sweep-9</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/6i0vykwk' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/6i0vykwk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 9.7 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 819 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 3.3 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 3.3 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 1.6 M  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "9.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.3 M     Total params\n",
      "37.152    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4221010c5e28447aa2f713bc9f247d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc05659d15bd4bf7a9742223c29dcc42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b968f36ee848998a61d0923eaeac8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2150d7eb5cd14ba9b27f04a3fcfb75f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03007427b32c4fe9bc05dc6a73e241e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf73f9999b4132bc7cae789cfab55b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4758d7cd5d54a189d40c781abb18c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50add7a38b8c4cd0b18e3447dabf2ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412feae916c54128baae26f999f7ef2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794915fd4d2e40ab8d79ab7cd21a77ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa41aa2a51a8401ba33260e7446bf770",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2bc8af087974433a1d1b141a12f1904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "def20a94bebc43aca78fb8f13fef45ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba42d7256604ee087ec398ecd0846c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63556edaacae44c1b0a0d23246cd9056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99669e198eb4301a1fb7daa9c618646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26481886b8149f28aabf2d1a93a4e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73981350d5e340a4ad7267eaf247c732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf62542020174b059e03c436dd5c99a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▄▄▄▅▅▅▆▆▇▇█</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▅▅▅▄▄▃▃▃▂▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▆▆▅▆▇▇█▇▇▆█▄▇▃</td></tr><tr><td>validation_loss</td><td>▇▆▅▄▃▃▃▂▂▂▂▂▃▁▆▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>train_accuracy</td><td>0.77547</td></tr><tr><td>train_loss</td><td>0.85841</td></tr><tr><td>trainer/global_step</td><td>4249</td></tr><tr><td>validation_accuracy</td><td>0.294</td></tr><tr><td>validation_loss</td><td>2.10064</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/6i0vykwk' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/6i0vykwk</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_191218-6i0vykwk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ccr8now6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_192816-ccr8now6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ccr8now6' target=\"_blank\">solar-sweep-10</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ccr8now6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ccr8now6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 294 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.221    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9411b506cbb64e3d821ad6014fbd284a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e864bb91ec4f7c8d0cbc721cab6a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefce4d0fdc4429a9d104e3621cd63cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca0890d7ae44163a2820001599feb31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "833eedfebfc942dab4bc8b67af743cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b26658a5b7524d078d8ae0171b623f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c40d38cdac45ddb335d0c1f91d9455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6939846e02a0407bb5a8295987a7dced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329dfc08fd774fe4bedf554fd7477c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b0d5ac396f44513b90a83476e3b62ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▃▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▃▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>validation_accuracy</td><td>▁▄▅▇▆▇▇█</td></tr><tr><td>validation_loss</td><td>█▅▄▃▃▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.3978</td></tr><tr><td>train_loss</td><td>1.71708</td></tr><tr><td>trainer/global_step</td><td>999</td></tr><tr><td>validation_accuracy</td><td>0.3335</td></tr><tr><td>validation_loss</td><td>1.88489</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ccr8now6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ccr8now6</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_192816-ccr8now6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: er8habcu with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_193814-er8habcu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/er8habcu' target=\"_blank\">worldly-sweep-11</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/er8habcu' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/er8habcu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650596ac81584a1d961bb08ae7d53fee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f32ce293d14b738fc1a94f7f783dca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/er8habcu' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/er8habcu</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_193814-er8habcu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run er8habcu errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/nadam.py\", line 167, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in closure\n",
      "    self._backward_fn(step_output.closure_loss)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 241, in backward_fn\n",
      "    call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 213, in backward\n",
      "    self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 73, in backward\n",
      "    model.backward(tensor, *args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1097, in backward\n",
      "    loss.backward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.01 GiB is free. Process 3444 has 14.88 GiB memory in use. Of the allocated memory 11.25 GiB is allocated by PyTorch, and 3.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run er8habcu errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/nadam.py\", line 167, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 140, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._backward_fn(step_output.closure_loss)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 241, in backward_fn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_strategy_hook(self.trainer, \"backward\", loss, optimizer)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 213, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 73, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     model.backward(tensor, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1097, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss.backward(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 581, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     torch.autograd.backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     _engine_run_backward(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 1.01 GiB is free. Process 3444 has 14.88 GiB memory in use. Of the allocated memory 11.25 GiB is allocated by PyTorch, and 3.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 06o5bykv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_193844-06o5bykv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/06o5bykv' target=\"_blank\">fast-sweep-12</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/06o5bykv' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/06o5bykv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a79b5153fe9f44aebfa5b1e0de57ecb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2e845be90b48aab19891f4212d0e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/06o5bykv' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/06o5bykv</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_193844-06o5bykv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 06o5bykv errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "    loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 155, in forward\n",
      "    x = self.activation_layer(x)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 133, in forward\n",
      "    return F.relu(input, inplace=self.inplace)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 1704, in relu\n",
      "    result = torch.relu(input)\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 139.12 MiB is free. Process 3444 has 15.75 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 393.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 06o5bykv errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._step_fn()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 155, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.activation_layer(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 133, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.relu(input, inplace=self.inplace)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 1704, in relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     result = torch.relu(input)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 282.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 139.12 MiB is free. Process 3444 has 15.75 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 393.49 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: emslsbgq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_193906-emslsbgq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/emslsbgq' target=\"_blank\">happy-sweep-13</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/emslsbgq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/emslsbgq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 9.7 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 819 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 3.3 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 3.3 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 1.6 M  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 65.6 K | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "9.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "9.1 M     Total params\n",
      "36.365    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91416d7e9c3495aa9b73c7b77671905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/emslsbgq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/emslsbgq</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_193906-emslsbgq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run emslsbgq errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 159.12 MiB is free. Process 3444 has 15.73 GiB memory in use. Of the allocated memory 15.10 GiB is allocated by PyTorch, and 342.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run emslsbgq errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 498.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 159.12 MiB is free. Process 3444 has 15.73 GiB memory in use. Of the allocated memory 15.10 GiB is allocated by PyTorch, and 342.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8xu8koya with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_193927-8xu8koya</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8xu8koya' target=\"_blank\">warm-sweep-14</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8xu8koya' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8xu8koya</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "14.058    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3304794b29941819735aa981a6535d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8xu8koya' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8xu8koya</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_193927-8xu8koya/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 8xu8koya errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 185.12 MiB is free. Process 3444 has 15.71 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 355.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8xu8koya errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 185.12 MiB is free. Process 3444 has 15.71 GiB memory in use. Of the allocated memory 15.06 GiB is allocated by PyTorch, and 355.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cka6ljm0 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_193958-cka6ljm0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cka6ljm0' target=\"_blank\">autumn-sweep-15</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cka6ljm0' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cka6ljm0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.139    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6febcd08e7b4e2a84d1ca78cecd1e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cka6ljm0' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cka6ljm0</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_193958-cka6ljm0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cka6ljm0 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 185.12 MiB is free. Process 3444 has 15.71 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 334.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cka6ljm0 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 185.12 MiB is free. Process 3444 has 15.71 GiB memory in use. Of the allocated memory 15.08 GiB is allocated by PyTorch, and 334.11 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 27hkhx1b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194023-27hkhx1b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/27hkhx1b' target=\"_blank\">gallant-sweep-16</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/27hkhx1b' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/27hkhx1b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 32.8 K | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.3 M     Total params\n",
      "9.173     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ca25f61d1f48c4b70c4258519acd1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c5dc3f17344064bd7f9cf86fe0e3ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_mish_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/27hkhx1b' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/27hkhx1b</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194023-27hkhx1b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 27hkhx1b errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "    self.fit_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "    self.advance()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "    self.epoch_loop.run(self._data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "    self.advance(data_fetcher)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "    self._optimizer_step(batch_idx, closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "    call._call_lightning_module_hook(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "    optimizer.step(closure=optimizer_closure)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "    return optimizer.step(closure=closure, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "    loss = closure()\n",
      "           ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "    closure_result = closure()\n",
      "                     ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "    self._result = self.closure(*args, **kwargs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "    step_output = self._step_fn()\n",
      "                  ^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "    return self.lightning_module.training_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "    loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 141, in forward\n",
      "    x = self.pool(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/pooling.py\", line 213, in forward\n",
      "    return F.max_pool2d(\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\", line 624, in fn\n",
      "    return if_false(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 830, in _max_pool2d\n",
      "    return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 5.12 MiB is free. Process 3444 has 15.88 GiB memory in use. Of the allocated memory 15.48 GiB is allocated by PyTorch, and 97.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 27hkhx1b errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1056, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.fit_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 216, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/fit_loop.py\", line 455, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.epoch_loop.run(self._data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 150, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.advance(data_fetcher)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/training_epoch_loop.py\", line 320, in advance\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 192, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._optimizer_step(batch_idx, closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 270, in _optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_lightning_module_hook(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 1302, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     optimizer.step(closure=optimizer_closure)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/optimizer.py\", line 154, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 239, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 123, in optimizer_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return optimizer.step(closure=closure, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 487, in wrapper\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     out = func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\", line 91, in _use_grad\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     ret = func(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/optim/adam.py\", line 202, in step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/plugins/precision/precision.py\", line 109, in _wrap_closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     closure_result = closure()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                      ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 146, in __call__\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._result = self.closure(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return func(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 131, in closure\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     step_output = self._step_fn()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/optimization/automatic.py\", line 319, in _training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 391, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.training_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 191, in training_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 141, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.pool(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/pooling.py\", line 213, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.max_pool2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/_jit_internal.py\", line 624, in fn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return if_false(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 830, in _max_pool2d\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 5.12 MiB is free. Process 3444 has 15.88 GiB memory in use. Of the allocated memory 15.48 GiB is allocated by PyTorch, and 97.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 66n28kkr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194044-66n28kkr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/66n28kkr' target=\"_blank\">eager-sweep-17</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/66n28kkr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/66n28kkr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 294 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.221    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8732a16a9c234a47af48c77bb0a3f4a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/66n28kkr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/66n28kkr</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194044-66n28kkr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 66n28kkr errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 104.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 66n28kkr errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 104.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ahx2tsfh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194105-ahx2tsfh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ahx2tsfh' target=\"_blank\">unique-sweep-18</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ahx2tsfh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ahx2tsfh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb09f633ae64f649fa8ac8c87145474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ahx2tsfh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ahx2tsfh</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194105-ahx2tsfh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ahx2tsfh errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 99.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ahx2tsfh errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 99.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bkmc9e1y with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194131-bkmc9e1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bkmc9e1y' target=\"_blank\">solar-sweep-19</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bkmc9e1y' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bkmc9e1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b637a6e0c8d74876a9301e8cc88c11bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bkmc9e1y' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bkmc9e1y</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194131-bkmc9e1y/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run bkmc9e1y errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.46 GiB is allocated by PyTorch, and 90.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run bkmc9e1y errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.46 GiB is allocated by PyTorch, and 90.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: grbwxlna with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194152-grbwxlna</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/grbwxlna' target=\"_blank\">pretty-sweep-20</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/grbwxlna' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/grbwxlna</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "946 K     Trainable params\n",
      "0         Non-trainable params\n",
      "946 K     Total params\n",
      "3.787     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5bf9cec694d4a8cb9b75f6ac8a15556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/grbwxlna' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/grbwxlna</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194152-grbwxlna/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run grbwxlna errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 98.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run grbwxlna errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.45 GiB is allocated by PyTorch, and 98.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: xehinx5r with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194213-xehinx5r</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/xehinx5r' target=\"_blank\">feasible-sweep-21</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/xehinx5r' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/xehinx5r</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff02ec67f1e4db795aac2dbaa96e0b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/xehinx5r' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/xehinx5r</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194213-xehinx5r/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run xehinx5r errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 84.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run xehinx5r errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.47 GiB is allocated by PyTorch, and 84.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: q2928ol6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194234-q2928ol6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/q2928ol6' target=\"_blank\">robust-sweep-22</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/q2928ol6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/q2928ol6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d33d1dd36636488497eaff02644eec22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/q2928ol6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/q2928ol6</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194234-q2928ol6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run q2928ol6 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.46 GiB is allocated by PyTorch, and 86.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run q2928ol6 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 266.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.46 GiB is allocated by PyTorch, and 86.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 47dkg2mr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194255-47dkg2mr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/47dkg2mr' target=\"_blank\">honest-sweep-23</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/47dkg2mr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/47dkg2mr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4ebbbdb571488c9d94120b66079c19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_relu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/47dkg2mr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/47dkg2mr</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194255-47dkg2mr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 47dkg2mr errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 48.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 47dkg2mr errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 37.12 MiB is free. Process 3444 has 15.85 GiB memory in use. Of the allocated memory 15.50 GiB is allocated by PyTorch, and 48.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wadiujjk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194316-wadiujjk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wadiujjk' target=\"_blank\">sunny-sweep-24</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wadiujjk' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wadiujjk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f362851cddaa486bbc3541b69e3901e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wadiujjk' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wadiujjk</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194316-wadiujjk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wadiujjk errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wadiujjk errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.02 GiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: l0ng7iz2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194336-l0ng7iz2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/l0ng7iz2' target=\"_blank\">fresh-sweep-25</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/l0ng7iz2' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/l0ng7iz2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.0 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.204    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3eafefa87bb462286783f875de609aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_relu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/l0ng7iz2' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/l0ng7iz2</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194336-l0ng7iz2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run l0ng7iz2 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 49.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run l0ng7iz2 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 49.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5bqy00gs with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194402-5bqy00gs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5bqy00gs' target=\"_blank\">logical-sweep-26</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5bqy00gs' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5bqy00gs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 294 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.221    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8392d923f24f4bd1bbf2abadd5e3287b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5bqy00gs' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5bqy00gs</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194402-5bqy00gs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5bqy00gs errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 56.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5bqy00gs errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 56.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: bxciaq1c with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194423-bxciaq1c</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bxciaq1c' target=\"_blank\">major-sweep-27</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bxciaq1c' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bxciaq1c</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bxciaq1c' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/bxciaq1c</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194423-bxciaq1c/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run bxciaq1c errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 51.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run bxciaq1c errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 51.12 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wgdcm1wn with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194449-wgdcm1wn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wgdcm1wn' target=\"_blank\">daily-sweep-28</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wgdcm1wn' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wgdcm1wn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73b662153544f178d25635f3c43e191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wgdcm1wn' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wgdcm1wn</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194449-wgdcm1wn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wgdcm1wn errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wgdcm1wn errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: e34d1koz with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194510-e34d1koz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/e34d1koz' target=\"_blank\">avid-sweep-29</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/e34d1koz' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/e34d1koz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b23a2bf64545fd82bcb42eab8a0f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/e34d1koz' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/e34d1koz</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194510-e34d1koz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run e34d1koz errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 60.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run e34d1koz errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.51 GiB is allocated by PyTorch, and 60.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: llpoa6eg with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194530-llpoa6eg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/llpoa6eg' target=\"_blank\">cool-sweep-30</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/llpoa6eg' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/llpoa6eg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b46ffb6136241dd8aac1b803b5ffdad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/llpoa6eg' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/llpoa6eg</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194530-llpoa6eg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run llpoa6eg errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run llpoa6eg errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 48.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: vdmdo46e with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194551-vdmdo46e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vdmdo46e' target=\"_blank\">balmy-sweep-31</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vdmdo46e' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vdmdo46e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54487cfe277f41e2bf656d6fb52ecb1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vdmdo46e' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/vdmdo46e</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194551-vdmdo46e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run vdmdo46e errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run vdmdo46e errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ok1v6xvr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194612-ok1v6xvr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ok1v6xvr' target=\"_blank\">hearty-sweep-32</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ok1v6xvr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ok1v6xvr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "14.058    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a50ae49d6af4ea1bb4071b857096ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ok1v6xvr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ok1v6xvr</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194612-ok1v6xvr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ok1v6xvr errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 40.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ok1v6xvr errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 40.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 59n2y5x2 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194633-59n2y5x2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/59n2y5x2' target=\"_blank\">autumn-sweep-33</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/59n2y5x2' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/59n2y5x2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/59n2y5x2' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/59n2y5x2</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194633-59n2y5x2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 59n2y5x2 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 51.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 59n2y5x2 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 51.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0we2v5w3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194657-0we2v5w3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0we2v5w3' target=\"_blank\">earthy-sweep-34</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0we2v5w3' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0we2v5w3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f224d788c84ea6a49355027452f384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0we2v5w3' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0we2v5w3</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194657-0we2v5w3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 0we2v5w3 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0we2v5w3 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cewqxynq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194719-cewqxynq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cewqxynq' target=\"_blank\">leafy-sweep-35</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cewqxynq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cewqxynq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4058e810970e448f87d93f2e7872df03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cewqxynq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cewqxynq</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194719-cewqxynq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cewqxynq errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 48.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cewqxynq errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 48.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: dsbdr6ue with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194740-dsbdr6ue</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/dsbdr6ue' target=\"_blank\">fluent-sweep-36</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/dsbdr6ue' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/dsbdr6ue</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.139    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1f0ca835334d9f9be86501936857fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_mish_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/dsbdr6ue' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/dsbdr6ue</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194740-dsbdr6ue/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run dsbdr6ue errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run dsbdr6ue errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qcurbv7w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194801-qcurbv7w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qcurbv7w' target=\"_blank\">fragrant-sweep-37</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qcurbv7w' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qcurbv7w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78db9eb676042419df9d83debfe75e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qcurbv7w' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qcurbv7w</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194801-qcurbv7w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run qcurbv7w errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 50.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qcurbv7w errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 50.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: umopv7zr with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194822-umopv7zr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/umopv7zr' target=\"_blank\">sleek-sweep-38</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/umopv7zr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/umopv7zr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "14.058    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d85086eb0643cca2376e92a9c1725b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_relu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/umopv7zr' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/umopv7zr</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194822-umopv7zr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run umopv7zr errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 40.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run umopv7zr errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 40.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: u9d5s5v9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194843-u9d5s5v9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/u9d5s5v9' target=\"_blank\">stellar-sweep-39</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/u9d5s5v9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/u9d5s5v9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "946 K     Trainable params\n",
      "0         Non-trainable params\n",
      "946 K     Total params\n",
      "3.787     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404063ae73524104978ac5f8993f9f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/u9d5s5v9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/u9d5s5v9</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194843-u9d5s5v9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run u9d5s5v9 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 50.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run u9d5s5v9 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.52 GiB is allocated by PyTorch, and 50.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: nvgh8e2o with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194904-nvgh8e2o</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/nvgh8e2o' target=\"_blank\">devout-sweep-40</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/nvgh8e2o' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/nvgh8e2o</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 294 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.221    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471f10358c32473e9ca8d37a64746c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/nvgh8e2o' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/nvgh8e2o</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194904-nvgh8e2o/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run nvgh8e2o errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run nvgh8e2o errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cmc61zqi with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194925-cmc61zqi</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cmc61zqi' target=\"_blank\">atomic-sweep-41</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cmc61zqi' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cmc61zqi</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6026cf0a0cd44f1cba240c2d14c0ea5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cmc61zqi' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cmc61zqi</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194925-cmc61zqi/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cmc61zqi errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cmc61zqi errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 134.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: y2nfpl5v with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_194946-y2nfpl5v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/y2nfpl5v' target=\"_blank\">soft-sweep-42</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/y2nfpl5v' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/y2nfpl5v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.359     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878aec9a6a994951b89f148898c36be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/y2nfpl5v' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/y2nfpl5v</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_194946-y2nfpl5v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run y2nfpl5v errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run y2nfpl5v errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: uhcxmuug with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195007-uhcxmuug</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/uhcxmuug' target=\"_blank\">crisp-sweep-43</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/uhcxmuug' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/uhcxmuug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/uhcxmuug' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/uhcxmuug</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195007-uhcxmuug/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run uhcxmuug errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run uhcxmuug errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2uqoozrp with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195032-2uqoozrp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2uqoozrp' target=\"_blank\">genial-sweep-44</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2uqoozrp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2uqoozrp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2uqoozrp' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2uqoozrp</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195032-2uqoozrp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2uqoozrp errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2uqoozrp errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2pggltsj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195049-2pggltsj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2pggltsj' target=\"_blank\">desert-sweep-45</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2pggltsj' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2pggltsj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.8 M     Total params\n",
      "11.139    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fc32c173e04ceeb684b9a5a108c150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_relu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2pggltsj' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2pggltsj</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195049-2pggltsj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2pggltsj errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2pggltsj errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 38.01 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 8wrsqj3w with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195109-8wrsqj3w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8wrsqj3w' target=\"_blank\">ethereal-sweep-46</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8wrsqj3w' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8wrsqj3w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.359     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd3f4b674b4340048da766e6d9bee57b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8wrsqj3w' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/8wrsqj3w</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195109-8wrsqj3w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 8wrsqj3w errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 8wrsqj3w errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0alqr2pm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195131-0alqr2pm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0alqr2pm' target=\"_blank\">glamorous-sweep-47</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0alqr2pm' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0alqr2pm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.359     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926cadfc564449ffb7c7f27db1ea66b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0alqr2pm' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/0alqr2pm</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195131-0alqr2pm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 0alqr2pm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0alqr2pm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 9plxpbjc with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195152-9plxpbjc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/9plxpbjc' target=\"_blank\">twilight-sweep-48</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/9plxpbjc' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/9plxpbjc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "082bc8bd00f24b2b8155680044da803d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/9plxpbjc' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/9plxpbjc</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195152-9plxpbjc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 9plxpbjc errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 9plxpbjc errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 44.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wocicnee with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195212-wocicnee</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wocicnee' target=\"_blank\">olive-sweep-49</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wocicnee' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wocicnee</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_relu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wocicnee' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wocicnee</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195212-wocicnee/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wocicnee errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wocicnee errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: o4g1u2rh with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195238-o4g1u2rh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/o4g1u2rh' target=\"_blank\">sleek-sweep-50</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/o4g1u2rh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/o4g1u2rh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/o4g1u2rh' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/o4g1u2rh</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195238-o4g1u2rh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run o4g1u2rh errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run o4g1u2rh errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: mw3gq039 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195254-mw3gq039</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/mw3gq039' target=\"_blank\">usual-sweep-51</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/mw3gq039' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/mw3gq039</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3115496637da4702aed6a80a379257f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/mw3gq039' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/mw3gq039</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195254-mw3gq039/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run mw3gq039 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run mw3gq039 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 5cusizh6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195325-5cusizh6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5cusizh6' target=\"_blank\">copper-sweep-52</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5cusizh6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5cusizh6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 524 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.3 M     Total params\n",
      "5.359     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c6655faa3ce4bb296b1098a514b56fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_silu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5cusizh6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/5cusizh6</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195325-5cusizh6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 5cusizh6 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 5cusizh6 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 43.05 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: jl3qy5va with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195346-jl3qy5va</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/jl3qy5va' target=\"_blank\">ethereal-sweep-53</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/jl3qy5va' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/jl3qy5va</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/jl3qy5va' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/jl3qy5va</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195346-jl3qy5va/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run jl3qy5va errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run jl3qy5va errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 45.63 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cd0ah4it with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195402-cd0ah4it</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cd0ah4it' target=\"_blank\">peachy-sweep-54</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cd0ah4it' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cd0ah4it</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "946 K     Trainable params\n",
      "0         Non-trainable params\n",
      "946 K     Total params\n",
      "3.787     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f49c54df6a4c07b2a56f246ae1c314",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_mish_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cd0ah4it' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/cd0ah4it</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195402-cd0ah4it/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run cd0ah4it errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "    loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "    scores = self.forward(x)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "    x = self.conv1(x)\n",
      "        ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "    return F.conv2d(\n",
      "           ^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 38.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run cd0ah4it errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 412, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self.lightning_module.validation_step(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 202, in validation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 227, in _common_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     scores = self.forward(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/1121025524.py\", line 137, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = self.conv1(x)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._call_impl(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return forward_call(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 554, in forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._conv_forward(input, self.weight, self.bias)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\", line 549, in _conv_forward\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return F.conv2d(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 38.55 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: x3cvdwf9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195423-x3cvdwf9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/x3cvdwf9' target=\"_blank\">chocolate-sweep-55</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/x3cvdwf9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/x3cvdwf9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de53d6ab66046feb82c70585b0d9550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/x3cvdwf9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/x3cvdwf9</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195423-x3cvdwf9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run x3cvdwf9 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 22.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run x3cvdwf9 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 22.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: m1xqozwf with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195443-m1xqozwf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/m1xqozwf' target=\"_blank\">polar-sweep-56</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/m1xqozwf' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/m1xqozwf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a2d742d9c54dd0a6385b37c991ccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/m1xqozwf' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/m1xqozwf</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195443-m1xqozwf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run m1xqozwf errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run m1xqozwf errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 2tma9xuv with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195505-2tma9xuv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2tma9xuv' target=\"_blank\">vocal-sweep-57</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2tma9xuv' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2tma9xuv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.3 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.6 M     Total params\n",
      "18.318    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bad51028244530997071fd650b3ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2tma9xuv' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/2tma9xuv</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195505-2tma9xuv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 2tma9xuv errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 22.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 2tma9xuv errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 22.52 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: euy4dj2k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195526-euy4dj2k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/euy4dj2k' target=\"_blank\">fragrant-sweep-58</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/euy4dj2k' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/euy4dj2k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 262 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "3.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.5 M     Total params\n",
      "14.058    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74d3813757bc434ca1792f2e5560dc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/euy4dj2k' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/euy4dj2k</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195526-euy4dj2k/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run euy4dj2k errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 27.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run euy4dj2k errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 27.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: et15iw49 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195546-et15iw49</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/et15iw49' target=\"_blank\">devout-sweep-59</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/et15iw49' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/et15iw49</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3192fe546168408c86afcaa38fa51c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/et15iw49' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/et15iw49</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195546-et15iw49/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run et15iw49 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run et15iw49 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pv8hl7z9 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195608-pv8hl7z9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/pv8hl7z9' target=\"_blank\">sandy-sweep-60</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/pv8hl7z9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/pv8hl7z9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ff63cbe107a4787b67d1766521de234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/pv8hl7z9' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/pv8hl7z9</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195608-pv8hl7z9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run pv8hl7z9 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run pv8hl7z9 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: hq968ti8 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195628-hq968ti8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hq968ti8' target=\"_blank\">kind-sweep-61</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hq968ti8' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hq968ti8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hq968ti8' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/hq968ti8</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195628-hq968ti8/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run hq968ti8 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run hq968ti8 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 7yh89c7b with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195654-7yh89c7b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/7yh89c7b' target=\"_blank\">silver-sweep-62</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/7yh89c7b' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/7yh89c7b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731d2535ba8d4e2b95a4beaf5303c407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/7yh89c7b' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/7yh89c7b</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195654-7yh89c7b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 7yh89c7b errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 7yh89c7b errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3cuozx4d with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: mish\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nadam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195715-3cuozx4d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3cuozx4d' target=\"_blank\">avid-sweep-63</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3cuozx4d' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3cuozx4d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | Mish             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 294 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.6 M     Total params\n",
      "10.221    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edb7afae8ef747e7b475647695412725",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_mish_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3cuozx4d' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3cuozx4d</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195715-3cuozx4d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 3cuozx4d errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 32.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 3cuozx4d errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 32.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wmpr43d4 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195736-wmpr43d4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wmpr43d4' target=\"_blank\">different-sweep-64</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wmpr43d4' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wmpr43d4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wmpr43d4' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/wmpr43d4</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195736-wmpr43d4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wmpr43d4 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wmpr43d4 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ukskvz36 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: rmsprop\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195752-ukskvz36</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ukskvz36' target=\"_blank\">cosmic-sweep-65</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ukskvz36' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ukskvz36</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_silu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ukskvz36' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/ukskvz36</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195752-ukskvz36/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ukskvz36 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ukskvz36 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qia6w1do with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195807-qia6w1do</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qia6w1do' target=\"_blank\">drawn-sweep-66</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qia6w1do' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qia6w1do</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | ReLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 663 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.5 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.5 M     Total params\n",
      "5.916     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15310de6f85a41eaa6c031ecf79bec7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_relu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qia6w1do' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qia6w1do</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195807-qia6w1do/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run qia6w1do errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qia6w1do errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 48.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 36.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qf6pf3gm with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195829-qf6pf3gm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qf6pf3gm' target=\"_blank\">likely-sweep-67</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qf6pf3gm' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qf6pf3gm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 4.9 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 204 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 819 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 819 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 409 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 131 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "2.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.4 M     Total params\n",
      "9.566     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af24a344f45478f8f6455b393b1d460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_5_fo_2_a_gelu_e_20_b_128_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qf6pf3gm' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/qf6pf3gm</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195829-qf6pf3gm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run qf6pf3gm errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run qf6pf3gm errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.54 GiB is allocated by PyTorch, and 33.51 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: td7vbpuq with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: sgd\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195859-td7vbpuq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/td7vbpuq' target=\"_blank\">sweepy-sweep-68</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/td7vbpuq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/td7vbpuq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_5_fo_2_a_gelu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/td7vbpuq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/td7vbpuq</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195859-td7vbpuq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run td7vbpuq errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "    self.strategy.setup(self)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "    self.model_to_device()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "    self.model.to(self.root_device)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "    return super().to(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "    return self._apply(convert)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "    param_applied = fn(param)\n",
      "                    ^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "    return t.to(\n",
      "           ^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run td7vbpuq errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 988, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.strategy.setup(self)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 155, in setup\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model_to_device()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/single_device.py\", line 79, in model_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self.model.to(self.root_device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/device_dtype_mixin.py\", line 55, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return super().to(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return self._apply(convert)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     module._apply(fn)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     param_applied = fn(param)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                     ^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return t.to(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 39.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 385gzbgt with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195915-385gzbgt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/385gzbgt' target=\"_blank\">decent-sweep-69</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/385gzbgt' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/385gzbgt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 1.8 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 128    | train\n",
      "3  | conv2            | Conv2d           | 73.9 K | train\n",
      "4  | bn2              | BatchNorm2d      | 256    | train\n",
      "5  | conv3            | Conv2d           | 295 K  | train\n",
      "6  | bn3              | BatchNorm2d      | 512    | train\n",
      "7  | conv4            | Conv2d           | 295 K  | train\n",
      "8  | bn4              | BatchNorm2d      | 256    | train\n",
      "9  | conv5            | Conv2d           | 147 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 256    | train\n",
      "11 | activation_layer | GELU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 204 K  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "1.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.0 M     Total params\n",
      "4.081     Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2baa1dda0c5d4e55ba5a49c6d3bae36a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/385gzbgt' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/385gzbgt</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195915-385gzbgt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 385gzbgt errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 38.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 385gzbgt errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.53 GiB is allocated by PyTorch, and 38.27 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 76lzf2c3 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: silu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250416_195941-76lzf2c3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76lzf2c3' target=\"_blank\">denim-sweep-70</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/0bgpvbyq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76lzf2c3' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76lzf2c3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO: \n",
      "   | Name             | Type             | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0  | loss_fn          | CrossEntropyLoss | 0      | train\n",
      "1  | conv1            | Conv2d           | 3.6 K  | train\n",
      "2  | bn1              | BatchNorm2d      | 256    | train\n",
      "3  | conv2            | Conv2d           | 295 K  | train\n",
      "4  | bn2              | BatchNorm2d      | 512    | train\n",
      "5  | conv3            | Conv2d           | 1.2 M  | train\n",
      "6  | bn3              | BatchNorm2d      | 1.0 K  | train\n",
      "7  | conv4            | Conv2d           | 1.2 M  | train\n",
      "8  | bn4              | BatchNorm2d      | 512    | train\n",
      "9  | conv5            | Conv2d           | 590 K  | train\n",
      "10 | bn5              | BatchNorm2d      | 512    | train\n",
      "11 | activation_layer | SiLU             | 0      | train\n",
      "12 | pool             | MaxPool2d        | 0      | train\n",
      "13 | dropout_layer    | Dropout          | 0      | train\n",
      "14 | flatten          | Flatten          | 0      | train\n",
      "15 | fc1              | Linear           | 1.0 M  | train\n",
      "16 | fc2              | Linear           | 650    | train\n",
      "17 | softmax_layer    | Softmax          | 0      | train\n",
      "---------------------------------------------------------------\n",
      "4.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.3 M     Total params\n",
      "17.204    Total estimated model params size (MB)\n",
      "18        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b14a6ca1334a84b08d7fd313665556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nf_128_fsz_3_fo_2_a_silu_e_20_b_32_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76lzf2c3' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/76lzf2c3</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250416_195941-76lzf2c3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run 76lzf2c3 errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "    trainer.fit(model, data)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "    batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "    return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "    batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "    return trainer_method(trainer, hook_name, *args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "    return move_data_to_device(batch, device)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "    return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "    return [function(x, *args, **kwargs) for x in data]\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "    data_output = data.to(device, **kwargs)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 24.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 76lzf2c3 errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/wandb/agents/pyagent.py\", line 306, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_31/546253167.py\", line 88, in train\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     trainer.fit(model, data)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     call._call_and_handle_interrupt(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 48, in _call_and_handle_interrupt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run(model, ckpt_path=ckpt_path)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     results = self._run_stage()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m               ^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._run_sanity_check()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_loop.run()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return loop_run(self, *args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 145, in run\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loops/evaluation_loop.py\", line 411, in _evaluation_step\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = call._call_strategy_hook(trainer, \"batch_to_device\", batch, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 328, in _call_strategy_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/strategies/strategy.py\", line 278, in batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return model._apply_batch_transfer_handler(batch, device=device, dataloader_idx=dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 352, in _apply_batch_transfer_handler\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     batch = self._call_batch_hook(\"transfer_batch_to_device\", batch, device, dataloader_idx)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/module.py\", line 341, in _call_batch_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return trainer_method(trainer, hook_name, *args)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/trainer/call.py\", line 176, in _call_lightning_module_hook\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     output = fn(*args, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m              ^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/pytorch/core/hooks.py\", line 611, in transfer_batch_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return move_data_to_device(batch, device)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 110, in move_data_to_device\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return apply_to_collection(batch, dtype=_TransferableDataType, function=batch_to)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in apply_to_collection\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning_utilities/core/apply_func.py\", line 68, in <listcomp>\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     return [function(x, *args, **kwargs) for x in data]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/usr/local/lib/python3.11/dist-packages/lightning/fabric/utilities/apply_func.py\", line 104, in batch_to\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     data_output = data.to(device, **kwargs)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 24.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 13.12 MiB is free. Process 3444 has 15.87 GiB memory in use. Of the allocated memory 15.55 GiB is allocated by PyTorch, and 24.40 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.fabric.utilities.seed import seed_everything\n",
    "from pathlib import Path\n",
    "\n",
    "#from convolutional import convNet\n",
    "#from dataset import iNaturalistDataModule\n",
    "#from wandbConfigs import wandbconfig\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "seed_everything(42)\n",
    "sweep_configs = {\n",
    "    \"name\": \"Larger Model Sweep(bayes)-3\",\n",
    "    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n",
    "    \"method\": \"bayes\",\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 2, \"eta\": 2},\n",
    "    \"parameters\": {\n",
    "        \"img_size\": {\"values\": [128,256]},\n",
    "        \"num_filters\": {\"values\": [64,128]},\n",
    "        \"filter_size\": {\"values\": [3, 5]},\n",
    "        \"filter_org\": {\"values\": [2]},\n",
    "        \"activation\": {\"values\": [\"relu\",\"gelu\",\"silu\",\"mish\"]},\n",
    "        \"optimizer\": {\"values\": [\"adam\",\"sgd\",\"nadam\",\"rmsprop\"]},\n",
    "        \"epochs\": {\"values\": [20]},\n",
    "        \"batch_size\": {\"values\": [32,64,128]},\n",
    "        \"learning_rate\": {\"values\": [0.001]},\n",
    "        \"dataug\": {\"values\": [\"N\"]},\n",
    "        \"dense_neurons\": {\"values\": [64]},\n",
    "        \"batchnorm\": {\"values\": [\"Y\"]},\n",
    "        \"stride\": {\"values\": [1]},\n",
    "        \"padding\": {\"values\": [1, 2]},\n",
    "        \"dropout\": {\"values\": [0.1,0.2,0.3,0.05]},\n",
    "        \"usedropout\": {\"values\": [\"Y\"]},\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "hyperparameter_defaults = dict()\n",
    "\n",
    "\n",
    "def train():\n",
    "\n",
    "    # Initialise wandb\n",
    "\n",
    "    wandb.init(\n",
    "        config=hyperparameter_defaults, project=\"CS23S025-Assignment-2-DL\", \n",
    "        entity=\"cs23s025-indian-institute-of-technology-madras\"\n",
    "    )\n",
    "    wandb_logger = WandbLogger(project=\"CS23S025-Assignment-2-DL\")\n",
    "    wandb_configs = wandb.config\n",
    "\n",
    "    sweep_name = wandbconfig.sweep_name(wandb_configs)\n",
    "\n",
    "    wandb.run.name = sweep_name\n",
    "\n",
    "    model = convNet(\n",
    "        img_size=wandb_configs.img_size,\n",
    "        activation=wandb_configs.activation,\n",
    "        num_filters=wandb_configs.num_filters,\n",
    "        filter_size=wandb_configs.filter_size,\n",
    "        filter_org=wandb_configs.filter_org,\n",
    "        stride=wandb_configs.stride,\n",
    "        padding=wandb_configs.padding,\n",
    "        dense_neurons=wandb_configs.dense_neurons,\n",
    "        learning_rate=wandb_configs.learning_rate,\n",
    "        optimizer=wandb_configs.optimizer,\n",
    "        dropout=wandb_configs.dropout,\n",
    "        usedropout=wandb_configs.usedropout,\n",
    "        batchnorm=wandb_configs.batchnorm,\n",
    "    )\n",
    "\n",
    "    data = iNaturalistDataModule(\n",
    "        data_dir=Path(\"/kaggle/input/nature/nature_12K/inaturalist_12K\"),\n",
    "        batch_size=wandb_configs.batch_size,\n",
    "        num_workers=2,\n",
    "        img_size=wandb_configs.img_size,\n",
    "        data_augmentation=wandb_configs.dataug,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        accelerator=device,\n",
    "        min_epochs=1,\n",
    "        max_epochs=wandb_configs.epochs,\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=50,\n",
    "    )\n",
    "    trainer.fit(model, data)\n",
    "    trainer.validate(model, data)\n",
    "    # trainer.test(model, data)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    sweep_id = wandb.sweep(sweep_configs,\n",
    "                           entity=\"cs23s025-indian-institute-of-technology-madras\",\n",
    "                           project=\"CS23S025-Assignment-2-DL\")\n",
    "    wandb.agent(sweep_id, function=train,count=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART-B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:59:52.871814Z",
     "iopub.status.busy": "2025-04-16T19:59:52.871518Z",
     "iopub.status.idle": "2025-04-16T19:59:52.876134Z",
     "shell.execute_reply": "2025-04-16T19:59:52.875351Z",
     "shell.execute_reply.started": "2025-04-16T19:59:52.871785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install lightning\n",
    "import wandb\n",
    "\n",
    "wandb.login(key = \"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:59:52.877181Z",
     "iopub.status.busy": "2025-04-16T19:59:52.876958Z",
     "iopub.status.idle": "2025-04-16T19:59:52.895789Z",
     "shell.execute_reply": "2025-04-16T19:59:52.895128Z",
     "shell.execute_reply.started": "2025-04-16T19:59:52.877162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class wandbconfig:\n",
    "\n",
    "    def set_configs(args):\n",
    "\n",
    "        wandb_configs = {\n",
    "            \"epochs\": args.epochs,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"dataug\": args.dataug,\n",
    "            \"batchnorm\": args.batchnorm,\n",
    "            \"num_filters\": args.num_filters,\n",
    "            \"filter_size\": args.filter_size,\n",
    "            \"filter_org\": args.filter_org,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"dropout\": args.dropout,\n",
    "            \"usedropout\": args.usedropout,\n",
    "            \"optimizer\": args.optimizer,\n",
    "            \"dense_neurons\": args.dense_neurons,\n",
    "            \"activation\": args.activation,\n",
    "            \"stride\": args.stride,\n",
    "            \"padding\": args.padding,\n",
    "        }\n",
    "\n",
    "        return wandb_configs\n",
    "\n",
    "    def run_name(wandb_configs):\n",
    "\n",
    "        run_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}_da_{}\".format(\n",
    "            wandb_configs[\"num_filters\"],\n",
    "            wandb_configs[\"filter_size\"],\n",
    "            wandb_configs[\"filter_org\"],\n",
    "            wandb_configs[\"activation\"],\n",
    "            wandb_configs[\"epochs\"],\n",
    "            wandb_configs[\"batch_size\"],\n",
    "            wandb_configs[\"dense_neurons\"],\n",
    "            wandb_configs[\"dataug\"],\n",
    "        )\n",
    "\n",
    "        return run_name\n",
    "\n",
    "    def sweep_name(wandb_configs):\n",
    "\n",
    "        sweep_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}\".format(\n",
    "            wandb_configs.num_filters,\n",
    "            wandb_configs.filter_size,\n",
    "            wandb_configs.filter_org,\n",
    "            wandb_configs.activation,\n",
    "            wandb_configs.epochs,\n",
    "            wandb_configs.batch_size,\n",
    "            wandb_configs.dense_neurons,\n",
    "        )\n",
    "\n",
    "        return sweep_name\n",
    "\n",
    "    def tuner_set_configs(args):\n",
    "\n",
    "        wandb_configs = {\n",
    "            \"epochs\": args.epochs,\n",
    "            \"img_size\": args.img_size,\n",
    "            \"batch_size\": args.batch_size,\n",
    "            \"learning_rate\": args.learning_rate,\n",
    "            \"dropout\": args.dropout,\n",
    "        }\n",
    "\n",
    "        return wandb_configs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:59:52.896805Z",
     "iopub.status.busy": "2025-04-16T19:59:52.896557Z",
     "iopub.status.idle": "2025-04-16T19:59:52.914845Z",
     "shell.execute_reply": "2025-04-16T19:59:52.914013Z",
     "shell.execute_reply.started": "2025-04-16T19:59:52.896780Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import lightning.pytorch as L\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "class iNaturalistDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size, num_workers, img_size, data_augmentation):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = data_dir / \"train\"\n",
    "        self.test_path = data_dir / \"val\"\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        if data_augmentation == \"Y\":\n",
    "\n",
    "            self.data_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size=(img_size, img_size)),\n",
    "                    transforms.AutoAugment(),  # This is the data augmentation method chosen\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n",
    "                    ),  # These values are calculated for our dataset\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        elif data_augmentation == \"N\":\n",
    "\n",
    "            self.data_transform = transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(size=(img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        self.test_data_transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=(img_size, img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.4748, 0.4645, 0.3965], std=[0.2004, 0.1954, 0.1923]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def setup(self, stage):\n",
    "        if stage == \"fit\":\n",
    "\n",
    "            # First load all the training data\n",
    "            train_data_full = datasets.ImageFolder(\n",
    "                root=self.train_path,\n",
    "                transform=self.data_transform,\n",
    "                target_transform=None,\n",
    "            )\n",
    "\n",
    "            # Decide the number of validation samples required\n",
    "            validation_samples_per_class = int(0.2 * 1000)\n",
    "\n",
    "            # These lists will hold the indices of training and validation data samples\n",
    "            train_indices = []\n",
    "            val_indices = []\n",
    "\n",
    "            for class_idx in range(len(train_data_full.classes)):\n",
    "\n",
    "                # Obtain the indices of each class\n",
    "                class_indices = [\n",
    "                    idx\n",
    "                    for idx, (_, label) in enumerate(train_data_full.imgs)\n",
    "                    if label == class_idx\n",
    "                ]\n",
    "\n",
    "                # Split and add the indices to the respective lists\n",
    "                val_indices.extend(class_indices[:validation_samples_per_class])\n",
    "                train_indices.extend(class_indices[validation_samples_per_class:])\n",
    "\n",
    "            # Create a two subsets of the initially loaded training data as training data and validation data\n",
    "            self.train_data = Subset(train_data_full, train_indices)\n",
    "            self.val_data = Subset(train_data_full, val_indices)\n",
    "\n",
    "        if stage == \"test\":\n",
    "            # Load the test data\n",
    "            self.test_data = datasets.ImageFolder(\n",
    "                root=self.test_path, transform=self.test_data_transform\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:59:52.916094Z",
     "iopub.status.busy": "2025-04-16T19:59:52.915842Z",
     "iopub.status.idle": "2025-04-16T19:59:52.934746Z",
     "shell.execute_reply": "2025-04-16T19:59:52.933983Z",
     "shell.execute_reply.started": "2025-04-16T19:59:52.916074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import lightning.pytorch as L\n",
    "import torchvision.models\n",
    "\n",
    "class efficientNet(L.LightningModule):\n",
    "\n",
    "    def __init__(self, dropout, learning_rate):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        weights = torchvision.models.EfficientNet_V2_S_Weights.DEFAULT #We obtain the best available weights for this model\n",
    "        model = torchvision.models.efficientnet_v2_s(weights=weights) #Initializing the model\n",
    "\n",
    "        #We use the approach of overwriting the classifier layer, so that output classes is 10.\n",
    "        model.classifier = torch.nn.Sequential(\n",
    "        torch.nn.Dropout(p=dropout, inplace=True), \n",
    "        torch.nn.Linear(in_features=1280, \n",
    "                        out_features=10, #Here initially it was 1000\n",
    "                        bias=True))\n",
    "\n",
    "        #We are freezing all the features layers. This does not include freezing of the classifier layer.\n",
    "        for param in model.features.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.efNet_tuned = model\n",
    "\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.efNet_tuned(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
    " \n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"train_loss\": loss,\n",
    "                \"train_accuracy\": accuracy\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"validation_loss\": loss,\n",
    "                \"validation_accuracy\": accuracy\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, _, _, accuracy = self._common_step(batch, batch_idx)\n",
    "        self.log_dict(\n",
    "            {\n",
    "                \"test_loss\": loss,\n",
    "                \"test_accuracy\": accuracy\n",
    "            },\n",
    "            on_step=False,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def _common_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        accuracy = 0\n",
    "        scores = self.forward(x)\n",
    "        loss = self.loss_fn(scores, y)\n",
    "        y_pred = torch.argmax(torch.softmax(scores, dim=1), dim=1)\n",
    "        accuracy += (y_pred == y).sum().item()\n",
    "        accuracy = accuracy/len(scores)\n",
    "        return loss, scores, y, accuracy\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        x, _ = batch\n",
    "        \n",
    "        scores = self.forward(x)\n",
    "        preds = torch.argmax(scores, dim=1)\n",
    "        return preds\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "         return optim.Adam(self.parameters(), lr=self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-16T19:59:52.936958Z",
     "iopub.status.busy": "2025-04-16T19:59:52.936765Z",
     "iopub.status.idle": "2025-04-16T19:59:52.954929Z",
     "shell.execute_reply": "2025-04-16T19:59:52.954335Z",
     "shell.execute_reply.started": "2025-04-16T19:59:52.936942Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import wandb\n",
    "from types import SimpleNamespace\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.fabric.utilities.seed import seed_everything\n",
    "from pathlib import Path\n",
    "\n",
    "# from efficientNet_v2 import efficientNet\n",
    "# from dataset import iNaturalistDataModule\n",
    "# from wandbConfigs import wandbconfig\n",
    "\n",
    "device = \"gpu\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "\n",
    "def train(wandb_configs, args):\n",
    "\n",
    "    # Initialise wandb\n",
    "    run_name = \"EfficientNetV2_Freeze_All-3\"\n",
    "    wandb.init(\n",
    "        config=wandb_configs,\n",
    "        project=args.wandb_project,\n",
    "        entity=args.wandb_entity,\n",
    "        name=run_name,\n",
    "    )\n",
    "    wandb_logger = WandbLogger(project=args.wandb_project)\n",
    "    wandb_configs = wandb.config\n",
    "\n",
    "    model = efficientNet(0.2, 0.001)\n",
    "\n",
    "    data = iNaturalistDataModule(\n",
    "        data_dir=Path(args.path),\n",
    "        batch_size=wandb_configs.batch_size,\n",
    "        num_workers=2,\n",
    "        img_size=wandb_configs.img_size,\n",
    "        data_augmentation=\"N\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        accelerator=device,\n",
    "        min_epochs=1,\n",
    "        max_epochs=wandb_configs.epochs,\n",
    "        logger=wandb_logger,\n",
    "        log_every_n_steps=50,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, data)\n",
    "    trainer.validate(model, data)\n",
    "    # trainer.test(model, data)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    args = SimpleNamespace(\n",
    "    wandb_project=\"CS23S025-Assignment-2-DL\",\n",
    "    wandb_entity=\"cs23s025-indian-institute-of-technology-madras\",\n",
    "    path=\"/kaggle/input/nature/nature_12K/inaturalist_12K\",\n",
    "    epochs=10,\n",
    "    img_size=224,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.001,\n",
    "    dropout=0.1,\n",
    "    )\n",
    "\n",
    "    # Get wandb configs\n",
    "    wandb_configs = wandbconfig.tuner_set_configs(args)\n",
    "    \n",
    "    # Run training\n",
    "    train(wandb_configs, args)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 7150963,
     "sourceId": 11417952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
