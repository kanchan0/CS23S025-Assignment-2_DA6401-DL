{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11423999,"sourceType":"datasetVersion","datasetId":7154606},{"sourceId":11465495,"sourceType":"datasetVersion","datasetId":7184888}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## PART-A","metadata":{}},{"cell_type":"code","source":"# import wandb\n# wandb.login(key = \"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n# ENTITY = \"cs23s025-indian-institute-of-technology-madras\"  \n# PROJECT = \"CS23S025-Assignment-2-DL\"  \n# API = wandb.Api()\n\n\n# runs = API.runs(f\"{ENTITY}/{PROJECT}\")\n# sweep_dict = {}\n\n# for run in runs:\n#     if run.sweep:  # Check if the run belongs to a sweep\n#         sweep_id = run.sweep.id\n#         sweep_name = run.sweep.config.get(\"name\", \"Unnamed Sweep\")\n#         sweep_dict[sweep_id] = sweep_name\n\n# for sweep_id, sweep_name in sweep_dict.items():\n#     print(f\"Sweep Name: {sweep_name}, Sweep ID: {sweep_id}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:38:54.730727Z","iopub.execute_input":"2025-04-19T12:38:54.731001Z","iopub.status.idle":"2025-04-19T12:38:54.735188Z","shell.execute_reply.started":"2025-04-19T12:38:54.730979Z","shell.execute_reply":"2025-04-19T12:38:54.734653Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!pip install lightning\nimport wandb\n\nwandb.login(key = \"b4dc866a06ba17317c20de0d13c1a64cc23096dd\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:38:54.740538Z","iopub.execute_input":"2025-04-19T12:38:54.740747Z","iopub.status.idle":"2025-04-19T12:40:14.143540Z","shell.execute_reply.started":"2025-04-19T12:38:54.740731Z","shell.execute_reply":"2025-04-19T12:40:14.142952Z"}},"outputs":[{"name":"stdout","text":"Collecting lightning\n  Downloading lightning-2.5.1-py3-none-any.whl.metadata (39 kB)\nRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning) (6.0.2)\nRequirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\nRequirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (0.14.3)\nRequirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (24.2)\nRequirement already satisfied: torch<4.0,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1+cu124)\nRequirement already satisfied: torchmetrics<3.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (1.7.1)\nRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.67.1)\nRequirement already satisfied: typing-extensions<6.0,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from lightning) (4.13.1)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning) (2.5.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.16)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (75.1.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch<4.0,>=2.1.0->lightning)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<4.0,>=2.1.0->lightning) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<4.0,>=2.1.0->lightning) (1.3.0)\nRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics<3.0,>=0.7.0->lightning) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.19.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (3.10)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>1.20.0->torchmetrics<3.0,>=0.7.0->lightning) (2024.2.0)\nDownloading lightning-2.5.1-py3-none-any.whl (818 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m818.9/818.9 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lightning\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.8.93\n    Uninstalling nvidia-nvjitlink-cu12-12.8.93:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.8.93\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.9.90\n    Uninstalling nvidia-curand-cu12-10.3.9.90:\n      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed lightning-2.5.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23s025\u001b[0m (\u001b[33mcs23s025-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport lightning.pytorch as L\n\n\nclass convNet(L.LightningModule):\n    def __init__(\n        self,\n        img_size,\n        activation,\n        num_filters,\n        filter_size,\n        filter_org,\n        stride,\n        padding,\n        dense_neurons,\n        learning_rate,\n        optimizer,\n        dropout,\n        usedropout,\n        batchnorm,\n    ):\n        super().__init__()\n\n        # self.img_size = img_size\n        self.activation = activation\n        self.num_filters = num_filters\n        self.kernel_size = filter_size\n        self.stride = stride\n        self.padding = padding\n        self.dense_neurons = dense_neurons\n        self.lr = learning_rate\n        self.optimizer = optimizer\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.usedropout = usedropout\n        self.batchnorm = batchnorm\n\n        # Define convolutional layers\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.num_filters,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn1 = nn.BatchNorm2d(num_features=self.num_filters)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=self.num_filters,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn2 = nn.BatchNorm2d(num_features=self.conv2.out_channels)\n\n        self.conv3 = nn.Conv2d(\n            in_channels=self.conv2.out_channels,\n            out_channels=self.conv2.out_channels * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn3 = nn.BatchNorm2d(num_features=self.conv3.out_channels)\n\n        self.conv4 = nn.Conv2d(\n            in_channels=self.conv3.out_channels,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn4 = nn.BatchNorm2d(num_features=self.conv4.out_channels)\n\n        self.conv5 = nn.Conv2d(\n            in_channels=self.conv4.out_channels,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn5 = nn.BatchNorm2d(num_features=self.conv5.out_channels)\n\n        # Define activation function based on user input\n        if self.activation.lower() == \"relu\":\n            self.activation_layer = nn.ReLU()\n        elif self.activation.lower() == \"gelu\":\n            self.activation_layer = nn.GELU()\n        elif self.activation.lower() == \"silu\":\n            self.activation_layer = nn.SiLU()\n        elif self.activation.lower() == \"mish\":\n            self.activation_layer = nn.Mish()\n        else:\n            raise ValueError(\n                \"Invalid activation function. Choose from 'relu', 'gelu', 'mish' or 'silu'\"\n            )\n\n        # Define max-pooling layers\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.dropout_layer = nn.Dropout(p=dropout)\n\n        self.flatten = nn.Flatten()\n\n        c1s = int(\n            (((img_size - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c2s = int(\n            (((c1s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c3s = int(\n            (((c2s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c4s = int(\n            (((c3s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c5s = int(\n            (((c4s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        # print(f\"c1s_{c1s}_c2s_{c2s}_c3s_{c3s}_c4s_{c4s}_c5s_{c5s}_out_{self.conv5.out_channels}\")\n\n        flatten_neurons = int(c5s * c5s * self.conv5.out_channels)\n\n        # print(f\"multi_{c5s*c5s*self.conv5.out_channels}_flat_{flatten_neurons}\")\n\n        # Define fully connected layers\n        self.fc1 = nn.Linear(flatten_neurons, dense_neurons)\n        self.fc2 = nn.Linear(dense_neurons, 10)\n        self.softmax_layer = nn.Softmax()\n\n    \n    def forward(self, x):\n        # Convolutional layers\n        x = self.conv1(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn1(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn2(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv3(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn3(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv4(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn4(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv5(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn5(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        # Flatten\n        x = self.flatten(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        # Fully connected layers\n        x = self.fc1(x)\n        x = self.activation_layer(x)\n        x = self.fc2(x)\n        # x = self.softmax_layer(x)\n\n        return x\n\n    def training_step(self, batch, batch_idx):\n        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n\n        self.log_dict(\n            {\"train_loss\": loss, \"train_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n\n    def validation_step(self, batch, batch_idx):\n        loss, _, _, accuracy = self._common_step(batch, batch_idx)\n        self.log_dict(\n            {\"validation_loss\": loss, \"validation_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n\n        self.log_dict(\n            {\"test_loss\": loss, \"test_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return loss\n\n    def _common_step(self, batch, batch_idx):\n        x, y = batch\n\n        accuracy = 0\n\n        scores = self.forward(x)\n        loss = self.loss_fn(scores, y)\n        y_pred = torch.argmax(torch.softmax(scores, dim=1), dim=1)\n        accuracy += (y_pred == y).sum().item()\n        accuracy = accuracy / len(scores)\n        return loss, scores, y, accuracy\n\n    def predict_step(self, batch, batch_idx):\n        x, _ = batch\n\n        scores = self.forward(x)\n        preds = torch.argmax(scores, dim=1)\n        return preds\n\n    def configure_optimizers(self):\n        if self.optimizer.lower() == \"adam\":\n\n            return optim.Adam(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"sgd\":\n\n            return optim.SGD(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"nadam\":\n\n            return optim.NAdam(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"rmsprop\":\n\n            return optim.RMSprop(self.parameters(), lr=self.lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:40:14.144962Z","iopub.execute_input":"2025-04-19T12:40:14.145285Z","iopub.status.idle":"2025-04-19T12:40:25.797731Z","shell.execute_reply.started":"2025-04-19T12:40:14.145267Z","shell.execute_reply":"2025-04-19T12:40:25.797186Z"},"_kg_hide-output":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import lightning.pytorch as L\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms\n\n\nclass iNaturalistDataModule(L.LightningDataModule):\n    def __init__(self, data_dir, batch_size, num_workers, img_size, data_augmentation):\n        super().__init__()\n\n        self.train_path = data_dir / \"train\"\n        self.test_path = data_dir / \"val\"\n        self.batch_size = batch_size\n        self.num_workers = num_workers\n\n        if data_augmentation == \"Y\":\n\n            self.data_transform = transforms.Compose(\n                [\n                    transforms.Resize(size=(img_size, img_size)),\n                    transforms.AutoAugment(),  # This is the data augmentation method chosen\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n                    ),  # These values are calculated for our dataset\n                ]\n            )\n\n        elif data_augmentation == \"N\":\n\n            self.data_transform = transforms.Compose(\n                [\n                    transforms.Resize(size=(img_size, img_size)),\n                    transforms.ToTensor(),\n                    transforms.Normalize(\n                        mean=[0.4712, 0.4600, 0.3896], std=[0.2034, 0.1981, 0.1948]\n                    ),\n                ]\n            )\n\n        self.test_data_transform = transforms.Compose(\n            [\n                transforms.Resize(size=(img_size, img_size)),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.4748, 0.4645, 0.3965], std=[0.2004, 0.1954, 0.1923]\n                ),\n            ]\n        )\n\n    def setup(self, stage):\n        if stage == \"fit\":\n\n            # First load all the training data\n            train_data_full = datasets.ImageFolder(\n                root=self.train_path,\n                transform=self.data_transform,\n                target_transform=None,\n            )\n\n            # Decide the number of validation samples required\n            validation_samples_per_class = int(0.2 * 1000)\n\n            # These lists will hold the indices of training and validation data samples\n            train_indices = []\n            val_indices = []\n\n            for class_idx in range(len(train_data_full.classes)):\n\n                # Obtain the indices of each class\n                class_indices = [\n                    idx\n                    for idx, (_, label) in enumerate(train_data_full.imgs)\n                    if label == class_idx\n                ]\n\n                # Split and add the indices to the respective lists\n                val_indices.extend(class_indices[:validation_samples_per_class])\n                train_indices.extend(class_indices[validation_samples_per_class:])\n\n            # Create a two subsets of the initially loaded training data as training data and validation data\n            self.train_data = Subset(train_data_full, train_indices)\n            self.val_data = Subset(train_data_full, val_indices)\n\n        if stage == \"test\":\n            # Load the test data\n            self.test_data = datasets.ImageFolder(\n                root=self.test_path, transform=self.test_data_transform\n            )\n\n    def train_dataloader(self):\n        return DataLoader(\n            self.train_data,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            shuffle=True,\n        )\n\n    def val_dataloader(self):\n        return DataLoader(\n            self.val_data,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            persistent_workers=True,\n            shuffle=False,\n        )\n\n    def test_dataloader(self):\n        return DataLoader(\n            self.test_data,\n            batch_size=self.batch_size,\n            num_workers=self.num_workers,\n            shuffle=False,\n        )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:40:25.798359Z","iopub.execute_input":"2025-04-19T12:40:25.798724Z","iopub.status.idle":"2025-04-19T12:40:25.808628Z","shell.execute_reply.started":"2025-04-19T12:40:25.798705Z","shell.execute_reply":"2025-04-19T12:40:25.807958Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class wandbconfig:\n\n    def set_configs(args):\n\n        wandb_configs = {\n            \"epochs\": args.epochs,\n            \"img_size\": args.img_size,\n            \"dataug\": args.dataug,\n            \"batchnorm\": args.batchnorm,\n            \"num_filters\": args.num_filters,\n            \"filter_size\": args.filter_size,\n            \"filter_org\": args.filter_org,\n            \"batch_size\": args.batch_size,\n            \"learning_rate\": args.learning_rate,\n            \"dropout\": args.dropout,\n            \"usedropout\": args.usedropout,\n            \"optimizer\": args.optimizer,\n            \"dense_neurons\": args.dense_neurons,\n            \"activation\": args.activation,\n            \"stride\": args.stride,\n            \"padding\": args.padding,\n        }\n\n        return wandb_configs\n\n    def run_name(wandb_configs):\n\n        run_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}_da_{}\".format(\n            wandb_configs[\"num_filters\"],\n            wandb_configs[\"filter_size\"],\n            wandb_configs[\"filter_org\"],\n            wandb_configs[\"activation\"],\n            wandb_configs[\"epochs\"],\n            wandb_configs[\"batch_size\"],\n            wandb_configs[\"dense_neurons\"],\n            wandb_configs[\"dataug\"],\n        )\n\n        return run_name\n\n    def sweep_name(wandb_configs):\n\n        sweep_name = \"nf_{}_fsz_{}_fo_{}_a_{}_e_{}_b_{}_dn_{}\".format(\n            wandb_configs.num_filters,\n            wandb_configs.filter_size,\n            wandb_configs.filter_org,\n            wandb_configs.activation,\n            wandb_configs.epochs,\n            wandb_configs.batch_size,\n            wandb_configs.dense_neurons,\n        )\n\n        return sweep_name\n\n    def tuner_set_configs(args):\n\n        wandb_configs = {\n            \"epochs\": args.epochs,\n            \"img_size\": args.img_size,\n            \"batch_size\": args.batch_size,\n            \"learning_rate\": args.learning_rate,\n            \"dropout\": args.dropout,\n        }\n\n        return wandb_configs\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:40:25.810584Z","iopub.execute_input":"2025-04-19T12:40:25.810794Z","iopub.status.idle":"2025-04-19T12:40:25.833878Z","shell.execute_reply.started":"2025-04-19T12:40:25.810776Z","shell.execute_reply":"2025-04-19T12:40:25.833252Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport wandb\nfrom lightning.pytorch.loggers import WandbLogger\nfrom lightning.pytorch import Trainer\nfrom lightning.fabric.utilities.seed import seed_everything\nfrom pathlib import Path\nfrom lightning.pytorch.callbacks import ModelCheckpoint\n#from convolutional import convNet\n#from dataset import iNaturalistDataModule\n#from wandbConfigs import wandbconfig\n\ndevice = \"gpu\" if torch.cuda.is_available() else \"cpu\"\nseed_everything(42)\nsweep_configs = {\n    \"name\": \"MAX Acc\",\n    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n    \"method\": \"bayes\",\n    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 4, \"eta\": 2},\n    \"parameters\": {\n        \"img_size\": {\"values\": [256]},\n        \"num_filters\": {\"values\": [64]},\n        \"filter_size\": {\"values\": [3]},\n        \"filter_org\": {\"values\": [2]},\n        \"activation\": {\"values\": [\"gelu\"]},\n        \"optimizer\": {\"values\": [\"adam\"]},\n        \"epochs\": {\"values\": [20]},\n        \"batch_size\": {\"values\": [64]},\n        \"learning_rate\": {\"values\": [0.001]},\n        \"dataug\": {\"values\": [\"N\"]},\n        \"dense_neurons\": {\"values\": [64]},\n        \"batchnorm\": {\"values\": [\"Y\"]},\n        \"stride\": {\"values\": [1]},\n        \"padding\": {\"values\": [1]},\n        \"dropout\": {\"values\": [0.1]},\n        \"usedropout\": {\"values\": [\"Y\"]},\n    },\n}\n\n\nhyperparameter_defaults = dict()\n\n\ndef train():\n\n    # Initialise wandb\n\n    wandb.init(\n        config=hyperparameter_defaults, project=\"CS23S025-Assignment-2-DL\", \n        entity=\"cs23s025-indian-institute-of-technology-madras\"\n    )\n    wandb_logger = WandbLogger(project=\"CS23S025-Assignment-2-DL\")\n    wandb_configs = wandb.config\n\n    sweep_name = wandbconfig.sweep_name(wandb_configs)\n\n    wandb.run.name = sweep_name\n\n    model = convNet(\n        img_size=wandb_configs.img_size,\n        activation=wandb_configs.activation,\n        num_filters=wandb_configs.num_filters,\n        filter_size=wandb_configs.filter_size,\n        filter_org=wandb_configs.filter_org,\n        stride=wandb_configs.stride,\n        padding=wandb_configs.padding,\n        dense_neurons=wandb_configs.dense_neurons,\n        learning_rate=wandb_configs.learning_rate,\n        optimizer=wandb_configs.optimizer,\n        dropout=wandb_configs.dropout,\n        usedropout=wandb_configs.usedropout,\n        batchnorm=wandb_configs.batchnorm,\n    )\n\n    data = iNaturalistDataModule(\n        data_dir=Path(\"/kaggle/input/naturenew/inaturalist_12K\"),\n        batch_size=wandb_configs.batch_size,\n        num_workers=2,\n        img_size=wandb_configs.img_size,\n        data_augmentation=wandb_configs.dataug,\n    )\n    \n    #\n\n    checkpoint_callback = ModelCheckpoint(\n    monitor=\"validation_accuracy\",\n    mode=\"max\",\n    save_top_k=1,\n    dirpath=\"/kaggle/working/output/\",\n    filename=\"best_model\",\n    save_weights_only=False  # Save full model\n    )\n    \n    trainer = Trainer(\n        accelerator=device,\n        callbacks=[checkpoint_callback],\n        min_epochs=1,\n        max_epochs=wandb_configs.epochs,\n        logger=wandb_logger,\n        log_every_n_steps=50,\n         \n    )\n    #trainer.fit(model, data)\n    trainer.fit(model, datamodule=data)\n    trainer.validate(model, data)\n    #trainer.test(model, data)\n    print(\"Best model path:\", checkpoint_callback.best_model_path)\n    best_model_path = checkpoint_callback.best_model_path\n   \n    wandb.finish()\n\nif __name__ == \"__main__\":\n\n    sweep_id = wandb.sweep(sweep_configs,\n                           entity=\"cs23s025-indian-institute-of-technology-madras\",\n                           project=\"CS23S025-Assignment-2-DL\")\n    wandb.agent(sweep_id, function=train,count=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T12:40:25.834746Z","iopub.execute_input":"2025-04-19T12:40:25.834995Z","iopub.status.idle":"2025-04-19T13:04:10.825013Z","shell.execute_reply.started":"2025-04-19T12:40:25.834971Z","shell.execute_reply":"2025-04-19T13:04:10.824209Z"}},"outputs":[{"name":"stderr","text":"INFO: Seed set to 42\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: jjtcz9g6\nSweep URL: https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/jjtcz9g6\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 3gvof987 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchnorm: Y\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdataug: N\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 20\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_org: 2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_size: 3\n\u001b[34m\u001b[1mwandb\u001b[0m: \timg_size: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tpadding: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tstride: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tusedropout: Y\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_124032-3gvof987</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3gvof987' target=\"_blank\">wobbly-sweep-1</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/jjtcz9g6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/jjtcz9g6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/jjtcz9g6' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/jjtcz9g6</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3gvof987' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3gvof987</a>"},"metadata":{}},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n/usr/local/lib/python3.11/dist-packages/lightning/pytorch/loggers/wandb.py:397: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\nINFO: \n   | Name             | Type             | Params | Mode \n---------------------------------------------------------------\n0  | loss_fn          | CrossEntropyLoss | 0      | train\n1  | conv1            | Conv2d           | 1.8 K  | train\n2  | bn1              | BatchNorm2d      | 128    | train\n3  | conv2            | Conv2d           | 73.9 K | train\n4  | bn2              | BatchNorm2d      | 256    | train\n5  | conv3            | Conv2d           | 295 K  | train\n6  | bn3              | BatchNorm2d      | 512    | train\n7  | conv4            | Conv2d           | 295 K  | train\n8  | bn4              | BatchNorm2d      | 256    | train\n9  | conv5            | Conv2d           | 147 K  | train\n10 | bn5              | BatchNorm2d      | 256    | train\n11 | activation_layer | GELU             | 0      | train\n12 | pool             | MaxPool2d        | 0      | train\n13 | dropout_layer    | Dropout          | 0      | train\n14 | flatten          | Flatten          | 0      | train\n15 | fc1              | Linear           | 524 K  | train\n16 | fc2              | Linear           | 650    | train\n17 | softmax_layer    | Softmax          | 0      | train\n---------------------------------------------------------------\n1.3 M     Trainable params\n0         Non-trainable params\n1.3 M     Total params\n5.359     Total estimated model params size (MB)\n18        Modules in train mode\n0         Modules in eval mode\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0da4526cfd94d978d0c6e0d079abb29"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_epochs=20` reached.\nINFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bc1bb155e774b3193d79c8f42370cb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36m   validation_accuracy   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.39399999380111694   \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36m     validation_loss     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.0338504314422607    \u001b[0m\u001b[35m \u001b[0m│\n└───────────────────────────┴───────────────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n│<span style=\"color: #008080; text-decoration-color: #008080\">    validation_accuracy    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.39399999380111694    </span>│\n│<span style=\"color: #008080; text-decoration-color: #008080\">      validation_loss      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    2.0338504314422607     </span>│\n└───────────────────────────┴───────────────────────────┘\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Best model path: /kaggle/working/output/best_model.ckpt\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▃▃▃▄▄▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>validation_accuracy</td><td>▁▃▄▄▅▅▆▆▆▆▆▇▇▇███▇▇██</td></tr><tr><td>validation_loss</td><td>█▆▄▅▃▃▂▂▂▂▃▂▁▃▂▃▂▅▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.66358</td></tr><tr><td>train_loss</td><td>0.94918</td></tr><tr><td>trainer/global_step</td><td>2500</td></tr><tr><td>validation_accuracy</td><td>0.394</td></tr><tr><td>validation_loss</td><td>2.03385</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">nf_64_fsz_3_fo_2_a_gelu_e_20_b_64_dn_64</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3gvof987' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/3gvof987</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_124032-3gvof987/logs</code>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"import torch\nfrom torch import nn, optim\nimport lightning.pytorch as L\n\n\nclass convNet(L.LightningModule):\n    def __init__(\n        self,\n        img_size,\n        activation,\n        num_filters,\n        filter_size,\n        filter_org,\n        stride,\n        padding,\n        dense_neurons,\n        learning_rate,\n        optimizer,\n        dropout,\n        usedropout,\n        batchnorm,\n    ):\n        super().__init__()\n\n        # self.img_size = img_size\n        self.activation = activation\n        self.num_filters = num_filters\n        self.kernel_size = filter_size\n        self.stride = stride\n        self.padding = padding\n        self.dense_neurons = dense_neurons\n        self.lr = learning_rate\n        self.optimizer = optimizer\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.usedropout = usedropout\n        self.batchnorm = batchnorm\n\n        # Define convolutional layers\n        self.conv1 = nn.Conv2d(\n            in_channels=3,\n            out_channels=self.num_filters,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn1 = nn.BatchNorm2d(num_features=self.num_filters)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=self.num_filters,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn2 = nn.BatchNorm2d(num_features=self.conv2.out_channels)\n\n        self.conv3 = nn.Conv2d(\n            in_channels=self.conv2.out_channels,\n            out_channels=self.conv2.out_channels * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn3 = nn.BatchNorm2d(num_features=self.conv3.out_channels)\n\n        self.conv4 = nn.Conv2d(\n            in_channels=self.conv3.out_channels,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn4 = nn.BatchNorm2d(num_features=self.conv4.out_channels)\n\n        self.conv5 = nn.Conv2d(\n            in_channels=self.conv4.out_channels,\n            out_channels=self.num_filters * filter_org,\n            kernel_size=self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        self.bn5 = nn.BatchNorm2d(num_features=self.conv5.out_channels)\n\n        # Define activation function based on user input\n        if self.activation.lower() == \"relu\":\n            self.activation_layer = nn.ReLU()\n        elif self.activation.lower() == \"gelu\":\n            self.activation_layer = nn.GELU()\n        elif self.activation.lower() == \"silu\":\n            self.activation_layer = nn.SiLU()\n        elif self.activation.lower() == \"mish\":\n            self.activation_layer = nn.Mish()\n        else:\n            raise ValueError(\n                \"Invalid activation function. Choose from 'relu', 'gelu', 'mish' or 'silu'\"\n            )\n\n        # Define max-pooling layers\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n        self.dropout_layer = nn.Dropout(p=dropout)\n\n        self.flatten = nn.Flatten()\n\n        c1s = int(\n            (((img_size - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c2s = int(\n            (((c1s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c3s = int(\n            (((c2s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c4s = int(\n            (((c3s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        c5s = int(\n            (((c4s - self.kernel_size + (2 * self.padding)) / self.stride) + 1) / 2\n        )\n\n        # print(f\"c1s_{c1s}_c2s_{c2s}_c3s_{c3s}_c4s_{c4s}_c5s_{c5s}_out_{self.conv5.out_channels}\")\n\n        flatten_neurons = int(c5s * c5s * self.conv5.out_channels)\n\n        # print(f\"multi_{c5s*c5s*self.conv5.out_channels}_flat_{flatten_neurons}\")\n\n        # Define fully connected layers\n        self.fc1 = nn.Linear(flatten_neurons, dense_neurons)\n        self.fc2 = nn.Linear(dense_neurons, 10)\n        self.softmax_layer = nn.Softmax()\n\n    \n    def forward(self, x):\n        # Convolutional layers\n        x = self.conv1(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn1(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        x = self.conv2(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn2(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv3(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn3(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv4(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn4(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        x = self.conv5(x)\n        if self.batchnorm == \"Y\":\n            x = self.bn5(x)\n        x = self.activation_layer(x)\n        x = self.pool(x)\n\n        # Flatten\n        x = self.flatten(x)\n\n        if self.usedropout == \"Y\":\n            x = self.dropout_layer(x)\n\n        # Fully connected layers\n        x = self.fc1(x)\n        x = self.activation_layer(x)\n        x = self.fc2(x)\n        # x = self.softmax_layer(x)\n\n        return x\n\n    def training_step(self, batch, batch_idx):\n        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n\n        self.log_dict(\n            {\"train_loss\": loss, \"train_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return {\"loss\": loss, \"scores\": scores, \"y\": y}\n\n    def validation_step(self, batch, batch_idx):\n        loss, _, _, accuracy = self._common_step(batch, batch_idx)\n        self.log_dict(\n            {\"validation_loss\": loss, \"validation_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return loss\n\n    def test_step(self, batch, batch_idx):\n        loss, scores, y, accuracy = self._common_step(batch, batch_idx)\n\n        self.log_dict(\n            {\"test_loss\": loss, \"test_accuracy\": accuracy},\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n        )\n        return loss\n\n    def _common_step(self, batch, batch_idx):\n        x, y = batch\n\n        accuracy = 0\n\n        scores = self.forward(x)\n        loss = self.loss_fn(scores, y)\n        y_pred = torch.argmax(torch.softmax(scores, dim=1), dim=1)\n        accuracy += (y_pred == y).sum().item()\n        accuracy = accuracy / len(scores)\n        return loss, scores, y, accuracy\n\n    def predict_step(self, batch, batch_idx):\n        x, _ = batch\n\n        scores = self.forward(x)\n        preds = torch.argmax(scores, dim=1)\n        return preds\n\n    def configure_optimizers(self):\n        if self.optimizer.lower() == \"adam\":\n\n            return optim.Adam(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"sgd\":\n\n            return optim.SGD(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"nadam\":\n\n            return optim.NAdam(self.parameters(), lr=self.lr)\n\n        elif self.optimizer.lower() == \"rmsprop\":\n\n            return optim.RMSprop(self.parameters(), lr=self.lr)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:04:10.826233Z","iopub.execute_input":"2025-04-19T13:04:10.826461Z","iopub.status.idle":"2025-04-19T13:04:10.848266Z","shell.execute_reply.started":"2025-04-19T13:04:10.826440Z","shell.execute_reply":"2025-04-19T13:04:10.847566Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import wandb\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import ImageFolder\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\n\n# Sweep function to call logging\ndef sweep_run():\n    wandb.init(\n        project=\"CS23S025-Assignment-2-DL\",\n        entity=\"cs23s025-indian-institute-of-technology-madras\",\n        name=\"predictionTableRun\",\n        notes=\"Logging prediction table and confusion matrix\",\n        tags=[\"prediction-table\", \"eval\", \"val-checkpoint\"]\n    )\n\n    # Define transformations\n    transform = transforms.Compose([\n        transforms.Resize((256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])\n\n    # Load validation dataset\n    val_dataset = ImageFolder(root=\"/kaggle/input/naturenew/inaturalist_12K/val\", transform=transform)\n    val_loader = DataLoader(val_dataset, batch_size=wandb.config.batch_size, shuffle=False, num_workers=4)\n    num_samples = 10\n\n    class_names = ['Amphibia', 'Animalia', 'Arachnida', 'Aves', 'Fungi', 'Insecta', 'Mammalia', 'Mollusca', 'Plantae', 'Reptilia']\n\n    # Load model\n    model = convNet.load_from_checkpoint(\n        checkpoint_path=\"/kaggle/working/output/best_model.ckpt\",\n        img_size=256,\n        activation=\"gelu\",\n        num_filters=64,\n        filter_size=3,\n        filter_org=2,\n        stride=1,\n        padding=1,\n        dense_neurons=64,\n        learning_rate=wandb.config.learning_rate,\n        optimizer=\"adam\",\n        dropout=0.1,\n        usedropout=\"Y\",\n        batchnorm=\"Y\"\n    )\n\n    # Move the model to the correct device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Log prediction table\n    def log_prediction_table():\n        model.eval()\n        columns = [\"Image\", \"Actual Label\", \"Predicted Label\"]\n        prediction_table = wandb.Table(columns=columns)\n\n        samples_logged = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                if samples_logged >= num_samples:\n                    break\n\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n                inputs_cpu = inputs.cpu()\n                labels_cpu = labels.cpu()\n                preds_cpu = preds.cpu()\n\n                for i in range(inputs_cpu.size(0)):\n                    if samples_logged >= num_samples:\n                        break\n\n                    img_tensor = inputs_cpu[i]\n                    true_label = class_names[labels_cpu[i].item()]\n                    pred_label = class_names[preds_cpu[i].item()]\n\n                    prediction_table.add_data(\n                        wandb.Image(img_tensor),\n                        true_label,\n                        pred_label\n                    )\n\n                    samples_logged += 1\n\n        if samples_logged > 0:\n            wandb.log({\"Prediction Table\": prediction_table})\n            print(f\"Logged prediction table with {samples_logged} samples.\")\n        else:\n            print(\"No samples were logged.\")\n\n    # Log confusion matrix\n    def log_confusion_matrix():\n        model.eval()\n        all_preds = []\n        all_labels = []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                outputs = model(inputs)\n                _, preds = torch.max(outputs, 1)\n\n                all_preds.extend(preds.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n\n        cm = confusion_matrix(all_labels, all_preds)\n\n        # Log confusion matrix to WandB\n        wandb.log({\n            \"confusion_matrix\": wandb.plot.confusion_matrix(\n                probs=None,\n                y_true=all_labels,\n                preds=all_preds,\n                class_names=class_names\n            )\n        })\n        print(\"Logged interactive confusion matrix.\")\n\n    # Call the logging functions\n    log_prediction_table()\n    log_confusion_matrix()\n    wandb.finish()\n\n# Sweep configuration with parameters\nsweep_config = {\n    \"name\": \"prediction-table-sweep\",\n    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n    \"method\": \"bayes\",\n    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 4, \"eta\": 2},\n    \"parameters\": {\n        \"learning_rate\": {\n            \"min\": 1e-5,\n            \"max\": 1e-2,\n            \"distribution\": \"uniform\"\n        },\n        \"batch_size\": {\n            \"values\": [16, 32, 64]\n        }\n    }\n}\n\n# Create the sweep\nsweep_id = wandb.sweep(sweep_config, entity=\"cs23s025-indian-institute-of-technology-madras\", project=\"CS23S025-Assignment-2-DL\")\n\n# Run the sweep agent\nwandb.agent(sweep_id, function=sweep_run,count =1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T13:15:48.338347Z","iopub.execute_input":"2025-04-19T13:15:48.338641Z","iopub.status.idle":"2025-04-19T13:16:18.709514Z","shell.execute_reply.started":"2025-04-19T13:15:48.338622Z","shell.execute_reply":"2025-04-19T13:16:18.708905Z"}},"outputs":[{"name":"stdout","text":"Create sweep with ID: cy140zka\nSweep URL: https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/cy140zka\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: w9wk4xtc with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.007246684251187438\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring project 'CS23S025-Assignment-2-DL' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Ignoring entity 'cs23s025-indian-institute-of-technology-madras' when running a sweep."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250419_131555-w9wk4xtc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/w9wk4xtc' target=\"_blank\">predictionTableRun</a></strong> to <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/cy140zka' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/cy140zka</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/cy140zka' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/sweeps/cy140zka</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/w9wk4xtc' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/w9wk4xtc</a>"},"metadata":{}},{"name":"stdout","text":"Logged prediction table with 10 samples.\nLogged interactive confusion matrix.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">predictionTableRun</strong> at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/w9wk4xtc' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL/runs/w9wk4xtc</a><br> View project at: <a href='https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL' target=\"_blank\">https://wandb.ai/cs23s025-indian-institute-of-technology-madras/CS23S025-Assignment-2-DL</a><br>Synced 5 W&B file(s), 2 media file(s), 14 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250419_131555-w9wk4xtc/logs</code>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}